{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f34dd8a-e994-4e45-8f4a-9b6ee5b93a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torch.optim import Adam\n",
    "from torchsummary import summary  # Import the summary function\n",
    "from PyTorchLabFlow import test_mods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f34f1bd-84d1-46a7-8a03-0f86f941d80b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc6bca7b-9d8c-4c8a-90c1-c87772741404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
      "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             ReLU-15          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
      "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "             ReLU-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
      "             ReLU-25          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
      "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
      "             ReLU-29           [-1, 64, 56, 56]               0\n",
      "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
      "             ReLU-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
      "             ReLU-35          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
      "             ReLU-39          [-1, 128, 56, 56]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "             ReLU-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-47          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-57          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
      "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
      "             ReLU-61          [-1, 128, 28, 28]               0\n",
      "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
      "             ReLU-64          [-1, 128, 28, 28]               0\n",
      "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-67          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
      "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
      "             ReLU-71          [-1, 128, 28, 28]               0\n",
      "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
      "             ReLU-74          [-1, 128, 28, 28]               0\n",
      "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-77          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
      "           Conv2d-79          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-80          [-1, 256, 28, 28]             512\n",
      "             ReLU-81          [-1, 256, 28, 28]               0\n",
      "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
      "             ReLU-84          [-1, 256, 14, 14]               0\n",
      "           Conv2d-85         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-87         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-89         [-1, 1024, 14, 14]               0\n",
      "       Bottleneck-90         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-91          [-1, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-92          [-1, 256, 14, 14]             512\n",
      "             ReLU-93          [-1, 256, 14, 14]               0\n",
      "           Conv2d-94          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-95          [-1, 256, 14, 14]             512\n",
      "             ReLU-96          [-1, 256, 14, 14]               0\n",
      "           Conv2d-97         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-99         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-100         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-101          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-102          [-1, 256, 14, 14]             512\n",
      "            ReLU-103          [-1, 256, 14, 14]               0\n",
      "          Conv2d-104          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-105          [-1, 256, 14, 14]             512\n",
      "            ReLU-106          [-1, 256, 14, 14]               0\n",
      "          Conv2d-107         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-109         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-110         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-111          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
      "            ReLU-113          [-1, 256, 14, 14]               0\n",
      "          Conv2d-114          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-115          [-1, 256, 14, 14]             512\n",
      "            ReLU-116          [-1, 256, 14, 14]               0\n",
      "          Conv2d-117         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-119         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-120         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-121          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-122          [-1, 256, 14, 14]             512\n",
      "            ReLU-123          [-1, 256, 14, 14]               0\n",
      "          Conv2d-124          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-125          [-1, 256, 14, 14]             512\n",
      "            ReLU-126          [-1, 256, 14, 14]               0\n",
      "          Conv2d-127         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-129         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
      "            ReLU-133          [-1, 256, 14, 14]               0\n",
      "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
      "            ReLU-136          [-1, 256, 14, 14]               0\n",
      "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-139         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-141          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-142          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-143          [-1, 512, 14, 14]               0\n",
      "          Conv2d-144            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-145            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-146            [-1, 512, 7, 7]               0\n",
      "          Conv2d-147           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-148           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-149           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-150           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-151           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-152           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-153            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-154            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-155            [-1, 512, 7, 7]               0\n",
      "          Conv2d-156            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-157            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-158            [-1, 512, 7, 7]               0\n",
      "          Conv2d-159           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-160           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-161           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-162           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-163            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-164            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-165            [-1, 512, 7, 7]               0\n",
      "          Conv2d-166            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-167            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-168            [-1, 512, 7, 7]               0\n",
      "          Conv2d-169           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-170           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-171           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-172           [-1, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
      "        Identity-174                 [-1, 2048]               0\n",
      "          ResNet-175                 [-1, 2048]               0\n",
      "          Linear-176                  [-1, 256]         524,544\n",
      "     BatchNorm1d-177                  [-1, 256]             512\n",
      "            ReLU-178                  [-1, 256]               0\n",
      "         Dropout-179                  [-1, 256]               0\n",
      "          Linear-180                   [-1, 74]          19,018\n",
      "================================================================\n",
      "Total params: 24,052,106\n",
      "Trainable params: 16,625,994\n",
      "Non-trainable params: 7,426,112\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 286.59\n",
      "Params size (MB): 91.75\n",
      "Estimated Total Size (MB): 378.92\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class ResN50_40(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResN50_40, self).__init__()\n",
    "\n",
    "        self.desc = \"[3,224,224] -> ResNet50preTrained(-40:)[2048] -> 256 -> 74\"\n",
    "\n",
    "        # Step 1: Load ResNet50 base model (without pre-trained weights)\n",
    "        self.base_model = models.resnet50(weights=None)  # No pretrained weights at first\n",
    "        self.base_model.fc = nn.Identity()  # Remove the final fully connected (fc) layer\n",
    "\n",
    "        # Step 2: Load the pre-trained weights into the base model\n",
    "        state_dict = torch.load(\"preTrained/resnet50_pretrained.pth\", weights_only=True)\n",
    "        self.base_model.load_state_dict(state_dict, strict=False)  # Load the pre-trained weights\n",
    "\n",
    "        # Custom layers after the base model\n",
    "        self.fc1 = nn.Linear(2048, 256)  # Fully connected layer\n",
    "        self.batch_norm = nn.BatchNorm1d(256)  # Apply BatchNorm after fc1\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(256, 74)  # Output layer (num_classes will be 74)\n",
    "\n",
    "        # Step 3: Create a list of all parameters\n",
    "        self.params = list(self.base_model.named_parameters())\n",
    "        # print(f\"Total parameters: {len(self.params)}\")\n",
    "\n",
    "        # Identify the last 10 layers\n",
    "        self.last_40_params = self.params[-40:]  # Get the last 10 parameters\n",
    "\n",
    "        # Step 4: Freeze all layers except the last 10 layers\n",
    "        for name, param in self.base_model.named_parameters():\n",
    "            param.requires_grad = False  # Freeze all layers initially\n",
    "\n",
    "        # Step 5: Unfreeze the last 10 layers\n",
    "        for name, param in self.last_40_params:\n",
    "            param.requires_grad = True  # Unfreeze the last 10 layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through ResNet50 base model\n",
    "        x = self.base_model(x)  # Get features from ResNet50\n",
    "        x = self.fc1(x)          # Apply fully connected layer 1\n",
    "        x = self.batch_norm(x)   # Apply batch normalization after fc1\n",
    "        x = self.relu(x)         # ReLU activation\n",
    "        x = self.dropout(x)      # Apply dropout\n",
    "        x = self.fc2(x)          # Output layer\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = ResN50_40()\n",
    "\n",
    "# Assuming 'device' is the variable where the device is specified, e.g., 'cuda' or 'cpu'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the appropriate device (GPU if available, otherwise CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "# Display model summary\n",
    "summary(model, input_size=(3, 224, 224))  # Input size (C, H, W) for ResNet50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f334b8ea-0fa7-458f-9449-181c473d5adc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
      "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             ReLU-15          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
      "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "             ReLU-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
      "             ReLU-25          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
      "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
      "             ReLU-29           [-1, 64, 56, 56]               0\n",
      "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
      "             ReLU-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
      "             ReLU-35          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
      "             ReLU-39          [-1, 128, 56, 56]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "             ReLU-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-47          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-57          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
      "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
      "             ReLU-61          [-1, 128, 28, 28]               0\n",
      "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
      "             ReLU-64          [-1, 128, 28, 28]               0\n",
      "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-67          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
      "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
      "             ReLU-71          [-1, 128, 28, 28]               0\n",
      "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
      "             ReLU-74          [-1, 128, 28, 28]               0\n",
      "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-77          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
      "           Conv2d-79          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-80          [-1, 256, 28, 28]             512\n",
      "             ReLU-81          [-1, 256, 28, 28]               0\n",
      "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
      "             ReLU-84          [-1, 256, 14, 14]               0\n",
      "           Conv2d-85         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-87         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-89         [-1, 1024, 14, 14]               0\n",
      "       Bottleneck-90         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-91          [-1, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-92          [-1, 256, 14, 14]             512\n",
      "             ReLU-93          [-1, 256, 14, 14]               0\n",
      "           Conv2d-94          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-95          [-1, 256, 14, 14]             512\n",
      "             ReLU-96          [-1, 256, 14, 14]               0\n",
      "           Conv2d-97         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-99         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-100         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-101          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-102          [-1, 256, 14, 14]             512\n",
      "            ReLU-103          [-1, 256, 14, 14]               0\n",
      "          Conv2d-104          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-105          [-1, 256, 14, 14]             512\n",
      "            ReLU-106          [-1, 256, 14, 14]               0\n",
      "          Conv2d-107         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-109         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-110         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-111          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
      "            ReLU-113          [-1, 256, 14, 14]               0\n",
      "          Conv2d-114          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-115          [-1, 256, 14, 14]             512\n",
      "            ReLU-116          [-1, 256, 14, 14]               0\n",
      "          Conv2d-117         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-119         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-120         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-121          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-122          [-1, 256, 14, 14]             512\n",
      "            ReLU-123          [-1, 256, 14, 14]               0\n",
      "          Conv2d-124          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-125          [-1, 256, 14, 14]             512\n",
      "            ReLU-126          [-1, 256, 14, 14]               0\n",
      "          Conv2d-127         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-129         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
      "            ReLU-133          [-1, 256, 14, 14]               0\n",
      "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
      "            ReLU-136          [-1, 256, 14, 14]               0\n",
      "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-139         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-141          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-142          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-143          [-1, 512, 14, 14]               0\n",
      "          Conv2d-144            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-145            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-146            [-1, 512, 7, 7]               0\n",
      "          Conv2d-147           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-148           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-149           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-150           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-151           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-152           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-153            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-154            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-155            [-1, 512, 7, 7]               0\n",
      "          Conv2d-156            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-157            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-158            [-1, 512, 7, 7]               0\n",
      "          Conv2d-159           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-160           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-161           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-162           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-163            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-164            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-165            [-1, 512, 7, 7]               0\n",
      "          Conv2d-166            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-167            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-168            [-1, 512, 7, 7]               0\n",
      "          Conv2d-169           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-170           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-171           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-172           [-1, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
      "        Identity-174                 [-1, 2048]               0\n",
      "          ResNet-175                 [-1, 2048]               0\n",
      "          Linear-176                  [-1, 256]         524,544\n",
      "     BatchNorm1d-177                  [-1, 256]             512\n",
      "            ReLU-178                  [-1, 256]               0\n",
      "         Dropout-179                  [-1, 256]               0\n",
      "          Linear-180                   [-1, 74]          19,018\n",
      "================================================================\n",
      "Total params: 24,052,106\n",
      "Trainable params: 24,052,106\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 286.59\n",
      "Params size (MB): 91.75\n",
      "Estimated Total Size (MB): 378.92\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class ResN50_un(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResN50_un, self).__init__()\n",
    "\n",
    "        self.desc = \"[3,224,224] -> ResNet50[2048] -> 256 -> 74\"\n",
    "\n",
    "        # Step 1: Load ResNet50 base model (without pre-trained weights)\n",
    "        self.base_model = models.resnet50(weights=None)  # No pretrained weights at first\n",
    "        self.base_model.fc = nn.Identity()  # Remove the final fully connected (fc) layer\n",
    "\n",
    "        # # Step 2: Load the pre-trained weights into the base model\n",
    "        # state_dict = torch.load(\"preTrained/resnet50_pretrained.pth\", weights_only=True)\n",
    "        # self.base_model.load_state_dict(state_dict, strict=False)  # Load the pre-trained weights\n",
    "\n",
    "        # Custom layers after the base model\n",
    "        self.fc1 = nn.Linear(2048, 256)  # Fully connected layer\n",
    "        self.batch_norm = nn.BatchNorm1d(256)  # Apply BatchNorm after fc1\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(256, 74)  # Output layer (num_classes will be 74)\n",
    "\n",
    "        # Step 3: Create a list of all parameters\n",
    "        # self.params = list(self.base_model.named_parameters())\n",
    "        # print(f\"Total parameters: {len(self.params)}\")\n",
    "\n",
    "        # Identify the last 10 layers\n",
    "        # self.last_35_params = self.params[-35:]  # Get the last 10 parameters\n",
    "\n",
    "        # Step 4: Freeze all layers except the last 10 layers\n",
    "        # for name, param in self.base_model.named_parameters():\n",
    "        #     param.requires_grad = False  # Freeze all layers initially\n",
    "\n",
    "        # # Step 5: Unfreeze the last 10 layers\n",
    "        # for name, param in self.last_35_params:\n",
    "        #     param.requires_grad = True  # Unfreeze the last 10 layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through ResNet50 base model\n",
    "        x = self.base_model(x)  # Get features from ResNet50\n",
    "        x = self.fc1(x)          # Apply fully connected layer 1\n",
    "        x = self.batch_norm(x)   # Apply batch normalization after fc1\n",
    "        x = self.relu(x)         # ReLU activation\n",
    "        x = self.dropout(x)      # Apply dropout\n",
    "        x = self.fc2(x)          # Output layer\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = ResN50_un()\n",
    "\n",
    "# Assuming 'device' is the variable where the device is specified, e.g., 'cuda' or 'cpu'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the appropriate device (GPU if available, otherwise CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "# Display model summary\n",
    "summary(model, input_size=(3, 224, 224))  # Input size (C, H, W) for ResNet50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f51c0a9d-c76e-41e3-a6eb-667e275fcaf4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
      "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             ReLU-15          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
      "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "             ReLU-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
      "             ReLU-25          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
      "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
      "             ReLU-29           [-1, 64, 56, 56]               0\n",
      "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
      "             ReLU-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
      "             ReLU-35          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
      "             ReLU-39          [-1, 128, 56, 56]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "             ReLU-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-47          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-57          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
      "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
      "             ReLU-61          [-1, 128, 28, 28]               0\n",
      "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
      "             ReLU-64          [-1, 128, 28, 28]               0\n",
      "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-67          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
      "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
      "             ReLU-71          [-1, 128, 28, 28]               0\n",
      "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
      "             ReLU-74          [-1, 128, 28, 28]               0\n",
      "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-77          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
      "           Conv2d-79          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-80          [-1, 256, 28, 28]             512\n",
      "             ReLU-81          [-1, 256, 28, 28]               0\n",
      "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
      "             ReLU-84          [-1, 256, 14, 14]               0\n",
      "           Conv2d-85         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-87         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-89         [-1, 1024, 14, 14]               0\n",
      "       Bottleneck-90         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-91          [-1, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-92          [-1, 256, 14, 14]             512\n",
      "             ReLU-93          [-1, 256, 14, 14]               0\n",
      "           Conv2d-94          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-95          [-1, 256, 14, 14]             512\n",
      "             ReLU-96          [-1, 256, 14, 14]               0\n",
      "           Conv2d-97         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-99         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-100         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-101          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-102          [-1, 256, 14, 14]             512\n",
      "            ReLU-103          [-1, 256, 14, 14]               0\n",
      "          Conv2d-104          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-105          [-1, 256, 14, 14]             512\n",
      "            ReLU-106          [-1, 256, 14, 14]               0\n",
      "          Conv2d-107         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-109         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-110         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-111          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
      "            ReLU-113          [-1, 256, 14, 14]               0\n",
      "          Conv2d-114          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-115          [-1, 256, 14, 14]             512\n",
      "            ReLU-116          [-1, 256, 14, 14]               0\n",
      "          Conv2d-117         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-119         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-120         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-121          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-122          [-1, 256, 14, 14]             512\n",
      "            ReLU-123          [-1, 256, 14, 14]               0\n",
      "          Conv2d-124          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-125          [-1, 256, 14, 14]             512\n",
      "            ReLU-126          [-1, 256, 14, 14]               0\n",
      "          Conv2d-127         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-129         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
      "            ReLU-133          [-1, 256, 14, 14]               0\n",
      "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
      "            ReLU-136          [-1, 256, 14, 14]               0\n",
      "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-139         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-141          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-142          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-143          [-1, 512, 14, 14]               0\n",
      "          Conv2d-144            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-145            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-146            [-1, 512, 7, 7]               0\n",
      "          Conv2d-147           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-148           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-149           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-150           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-151           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-152           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-153            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-154            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-155            [-1, 512, 7, 7]               0\n",
      "          Conv2d-156            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-157            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-158            [-1, 512, 7, 7]               0\n",
      "          Conv2d-159           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-160           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-161           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-162           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-163            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-164            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-165            [-1, 512, 7, 7]               0\n",
      "          Conv2d-166            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-167            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-168            [-1, 512, 7, 7]               0\n",
      "          Conv2d-169           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-170           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-171           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-172           [-1, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
      "        Identity-174                 [-1, 2048]               0\n",
      "          ResNet-175                 [-1, 2048]               0\n",
      "          Linear-176                  [-1, 256]         524,544\n",
      "     BatchNorm1d-177                  [-1, 256]             512\n",
      "            ReLU-178                  [-1, 256]               0\n",
      "         Dropout-179                  [-1, 256]               0\n",
      "          Linear-180                   [-1, 74]          19,018\n",
      "================================================================\n",
      "Total params: 24,052,106\n",
      "Trainable params: 15,773,514\n",
      "Non-trainable params: 8,278,592\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 286.59\n",
      "Params size (MB): 91.75\n",
      "Estimated Total Size (MB): 378.92\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class ResN50_35(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResN50_35, self).__init__()\n",
    "\n",
    "        self.desc = \"[3,224,224] -> ResNet50preTrained(-35:)[2048] -> 256 -> 74\"\n",
    "\n",
    "        # Step 1: Load ResNet50 base model (without pre-trained weights)\n",
    "        self.base_model = models.resnet50(weights=None)  # No pretrained weights at first\n",
    "        self.base_model.fc = nn.Identity()  # Remove the final fully connected (fc) layer\n",
    "\n",
    "        # Step 2: Load the pre-trained weights into the base model\n",
    "        state_dict = torch.load(\"preTrained/resnet50_pretrained.pth\", weights_only=True)\n",
    "        self.base_model.load_state_dict(state_dict, strict=False)  # Load the pre-trained weights\n",
    "\n",
    "        # Custom layers after the base model\n",
    "        self.fc1 = nn.Linear(2048, 256)  # Fully connected layer\n",
    "        self.batch_norm = nn.BatchNorm1d(256)  # Apply BatchNorm after fc1\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(256, 74)  # Output layer (num_classes will be 74)\n",
    "\n",
    "        # Step 3: Create a list of all parameters\n",
    "        self.params = list(self.base_model.named_parameters())\n",
    "        # print(f\"Total parameters: {len(self.params)}\")\n",
    "\n",
    "        # Identify the last 10 layers\n",
    "        self.last_35_params = self.params[-35:]  # Get the last 10 parameters\n",
    "\n",
    "        # Step 4: Freeze all layers except the last 10 layers\n",
    "        for name, param in self.base_model.named_parameters():\n",
    "            param.requires_grad = False  # Freeze all layers initially\n",
    "\n",
    "        # Step 5: Unfreeze the last 10 layers\n",
    "        for name, param in self.last_35_params:\n",
    "            param.requires_grad = True  # Unfreeze the last 10 layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through ResNet50 base model\n",
    "        x = self.base_model(x)  # Get features from ResNet50\n",
    "        x = self.fc1(x)          # Apply fully connected layer 1\n",
    "        x = self.batch_norm(x)   # Apply batch normalization after fc1\n",
    "        x = self.relu(x)         # ReLU activation\n",
    "        x = self.dropout(x)      # Apply dropout\n",
    "        x = self.fc2(x)          # Output layer\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = ResN50_35()\n",
    "\n",
    "# Assuming 'device' is the variable where the device is specified, e.g., 'cuda' or 'cpu'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the appropriate device (GPU if available, otherwise CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "# Display model summary\n",
    "summary(model, input_size=(3, 224, 224))  # Input size (C, H, W) for ResNet50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4749eca0-2720-4eff-b46a-0dfcbfe2ad9d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
      "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             ReLU-15          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
      "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "             ReLU-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
      "             ReLU-25          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
      "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
      "             ReLU-29           [-1, 64, 56, 56]               0\n",
      "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
      "             ReLU-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
      "             ReLU-35          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
      "             ReLU-39          [-1, 128, 56, 56]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "             ReLU-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-47          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-57          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
      "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
      "             ReLU-61          [-1, 128, 28, 28]               0\n",
      "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
      "             ReLU-64          [-1, 128, 28, 28]               0\n",
      "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-67          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
      "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
      "             ReLU-71          [-1, 128, 28, 28]               0\n",
      "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
      "             ReLU-74          [-1, 128, 28, 28]               0\n",
      "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-77          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
      "           Conv2d-79          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-80          [-1, 256, 28, 28]             512\n",
      "             ReLU-81          [-1, 256, 28, 28]               0\n",
      "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
      "             ReLU-84          [-1, 256, 14, 14]               0\n",
      "           Conv2d-85         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-87         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-89         [-1, 1024, 14, 14]               0\n",
      "       Bottleneck-90         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-91          [-1, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-92          [-1, 256, 14, 14]             512\n",
      "             ReLU-93          [-1, 256, 14, 14]               0\n",
      "           Conv2d-94          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-95          [-1, 256, 14, 14]             512\n",
      "             ReLU-96          [-1, 256, 14, 14]               0\n",
      "           Conv2d-97         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-99         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-100         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-101          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-102          [-1, 256, 14, 14]             512\n",
      "            ReLU-103          [-1, 256, 14, 14]               0\n",
      "          Conv2d-104          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-105          [-1, 256, 14, 14]             512\n",
      "            ReLU-106          [-1, 256, 14, 14]               0\n",
      "          Conv2d-107         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-109         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-110         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-111          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
      "            ReLU-113          [-1, 256, 14, 14]               0\n",
      "          Conv2d-114          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-115          [-1, 256, 14, 14]             512\n",
      "            ReLU-116          [-1, 256, 14, 14]               0\n",
      "          Conv2d-117         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-119         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-120         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-121          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-122          [-1, 256, 14, 14]             512\n",
      "            ReLU-123          [-1, 256, 14, 14]               0\n",
      "          Conv2d-124          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-125          [-1, 256, 14, 14]             512\n",
      "            ReLU-126          [-1, 256, 14, 14]               0\n",
      "          Conv2d-127         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-129         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
      "            ReLU-133          [-1, 256, 14, 14]               0\n",
      "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
      "            ReLU-136          [-1, 256, 14, 14]               0\n",
      "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-139         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-141          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-142          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-143          [-1, 512, 14, 14]               0\n",
      "          Conv2d-144            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-145            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-146            [-1, 512, 7, 7]               0\n",
      "          Conv2d-147           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-148           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-149           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-150           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-151           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-152           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-153            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-154            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-155            [-1, 512, 7, 7]               0\n",
      "          Conv2d-156            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-157            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-158            [-1, 512, 7, 7]               0\n",
      "          Conv2d-159           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-160           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-161           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-162           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-163            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-164            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-165            [-1, 512, 7, 7]               0\n",
      "          Conv2d-166            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-167            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-168            [-1, 512, 7, 7]               0\n",
      "          Conv2d-169           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-170           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-171           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-172           [-1, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
      "        Identity-174                 [-1, 2048]               0\n",
      "          ResNet-175                 [-1, 2048]               0\n",
      "          Linear-176                  [-1, 256]         524,544\n",
      "     BatchNorm1d-177                  [-1, 256]             512\n",
      "            ReLU-178                  [-1, 256]               0\n",
      "         Dropout-179                  [-1, 256]               0\n",
      "          Linear-180                   [-1, 74]          19,018\n",
      "================================================================\n",
      "Total params: 24,052,106\n",
      "Trainable params: 12,623,178\n",
      "Non-trainable params: 11,428,928\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 286.59\n",
      "Params size (MB): 91.75\n",
      "Estimated Total Size (MB): 378.92\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class ResN50_25(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResN50_25, self).__init__()\n",
    "\n",
    "        self.desc = \"[3,224,224] -> ResNet50preTrained(-15:)[2048] -> 256 -> 74\"\n",
    "\n",
    "        # Step 1: Load ResNet50 base model (without pre-trained weights)\n",
    "        self.base_model = models.resnet50(weights=None)  # No pretrained weights at first\n",
    "        self.base_model.fc = nn.Identity()  # Remove the final fully connected (fc) layer\n",
    "\n",
    "        # Step 2: Load the pre-trained weights into the base model\n",
    "        state_dict = torch.load(\"preTrained/resnet50_pretrained.pth\", weights_only=True)\n",
    "        self.base_model.load_state_dict(state_dict, strict=False)  # Load the pre-trained weights\n",
    "\n",
    "        # Custom layers after the base model\n",
    "        self.fc1 = nn.Linear(2048, 256)  # Fully connected layer\n",
    "        self.batch_norm = nn.BatchNorm1d(256)  # Apply BatchNorm after fc1\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(256, 74)  # Output layer (num_classes will be 74)\n",
    "\n",
    "        # Step 3: Create a list of all parameters\n",
    "        self.params = list(self.base_model.named_parameters())\n",
    "        # print(f\"Total parameters: {len(self.params)}\")\n",
    "\n",
    "        # Identify the last 10 layers\n",
    "        self.last_25_params = self.params[-25:]  # Get the last 10 parameters\n",
    "\n",
    "        # Step 4: Freeze all layers except the last 10 layers\n",
    "        for name, param in self.base_model.named_parameters():\n",
    "            param.requires_grad = False  # Freeze all layers initially\n",
    "\n",
    "        # Step 5: Unfreeze the last 10 layers\n",
    "        for name, param in self.last_25_params:\n",
    "            param.requires_grad = True  # Unfreeze the last 10 layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through ResNet50 base model\n",
    "        x = self.base_model(x)  # Get features from ResNet50\n",
    "        x = self.fc1(x)          # Apply fully connected layer 1\n",
    "        x = self.batch_norm(x)   # Apply batch normalization after fc1\n",
    "        x = self.relu(x)         # ReLU activation\n",
    "        x = self.dropout(x)      # Apply dropout\n",
    "        x = self.fc2(x)          # Output layer\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = ResN50_25()\n",
    "\n",
    "# Assuming 'device' is the variable where the device is specified, e.g., 'cuda' or 'cpu'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the appropriate device (GPU if available, otherwise CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "# Display model summary\n",
    "summary(model, input_size=(3, 224, 224))  # Input size (C, H, W) for ResNet50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1880312-a922-4b8d-99b8-ad40f5fa0fb4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
      "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             ReLU-15          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
      "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "             ReLU-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
      "             ReLU-25          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
      "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
      "             ReLU-29           [-1, 64, 56, 56]               0\n",
      "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
      "             ReLU-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
      "             ReLU-35          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
      "             ReLU-39          [-1, 128, 56, 56]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "             ReLU-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-47          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-57          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
      "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
      "             ReLU-61          [-1, 128, 28, 28]               0\n",
      "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
      "             ReLU-64          [-1, 128, 28, 28]               0\n",
      "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-67          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
      "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
      "             ReLU-71          [-1, 128, 28, 28]               0\n",
      "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
      "             ReLU-74          [-1, 128, 28, 28]               0\n",
      "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-77          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
      "           Conv2d-79          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-80          [-1, 256, 28, 28]             512\n",
      "             ReLU-81          [-1, 256, 28, 28]               0\n",
      "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
      "             ReLU-84          [-1, 256, 14, 14]               0\n",
      "           Conv2d-85         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-87         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-89         [-1, 1024, 14, 14]               0\n",
      "       Bottleneck-90         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-91          [-1, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-92          [-1, 256, 14, 14]             512\n",
      "             ReLU-93          [-1, 256, 14, 14]               0\n",
      "           Conv2d-94          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-95          [-1, 256, 14, 14]             512\n",
      "             ReLU-96          [-1, 256, 14, 14]               0\n",
      "           Conv2d-97         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-99         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-100         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-101          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-102          [-1, 256, 14, 14]             512\n",
      "            ReLU-103          [-1, 256, 14, 14]               0\n",
      "          Conv2d-104          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-105          [-1, 256, 14, 14]             512\n",
      "            ReLU-106          [-1, 256, 14, 14]               0\n",
      "          Conv2d-107         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-109         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-110         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-111          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
      "            ReLU-113          [-1, 256, 14, 14]               0\n",
      "          Conv2d-114          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-115          [-1, 256, 14, 14]             512\n",
      "            ReLU-116          [-1, 256, 14, 14]               0\n",
      "          Conv2d-117         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-119         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-120         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-121          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-122          [-1, 256, 14, 14]             512\n",
      "            ReLU-123          [-1, 256, 14, 14]               0\n",
      "          Conv2d-124          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-125          [-1, 256, 14, 14]             512\n",
      "            ReLU-126          [-1, 256, 14, 14]               0\n",
      "          Conv2d-127         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-129         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
      "            ReLU-133          [-1, 256, 14, 14]               0\n",
      "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
      "            ReLU-136          [-1, 256, 14, 14]               0\n",
      "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-139         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-141          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-142          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-143          [-1, 512, 14, 14]               0\n",
      "          Conv2d-144            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-145            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-146            [-1, 512, 7, 7]               0\n",
      "          Conv2d-147           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-148           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-149           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-150           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-151           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-152           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-153            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-154            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-155            [-1, 512, 7, 7]               0\n",
      "          Conv2d-156            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-157            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-158            [-1, 512, 7, 7]               0\n",
      "          Conv2d-159           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-160           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-161           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-162           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-163            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-164            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-165            [-1, 512, 7, 7]               0\n",
      "          Conv2d-166            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-167            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-168            [-1, 512, 7, 7]               0\n",
      "          Conv2d-169           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-170           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-171           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-172           [-1, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
      "        Identity-174                 [-1, 2048]               0\n",
      "          ResNet-175                 [-1, 2048]               0\n",
      "          Linear-176                  [-1, 256]         524,544\n",
      "     BatchNorm1d-177                  [-1, 256]             512\n",
      "            ReLU-178                  [-1, 256]               0\n",
      "         Dropout-179                  [-1, 256]               0\n",
      "          Linear-180                   [-1, 74]          19,018\n",
      "================================================================\n",
      "Total params: 24,052,106\n",
      "Trainable params: 9,473,354\n",
      "Non-trainable params: 14,578,752\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 286.59\n",
      "Params size (MB): 91.75\n",
      "Estimated Total Size (MB): 378.92\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class ResN50_20(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResN50_20, self).__init__()\n",
    "\n",
    "        self.desc = \"[3,224,224] -> ResNet50preTrained(-15:)[2048] -> 256 -> 74\"\n",
    "\n",
    "        # Step 1: Load ResNet50 base model (without pre-trained weights)\n",
    "        self.base_model = models.resnet50(weights=None)  # No pretrained weights at first\n",
    "        self.base_model.fc = nn.Identity()  # Remove the final fully connected (fc) layer\n",
    "\n",
    "        # Step 2: Load the pre-trained weights into the base model\n",
    "        state_dict = torch.load(\"preTrained/resnet50_pretrained.pth\", weights_only=True)\n",
    "        self.base_model.load_state_dict(state_dict, strict=False)  # Load the pre-trained weights\n",
    "\n",
    "        # Custom layers after the base model\n",
    "        self.fc1 = nn.Linear(2048, 256)  # Fully connected layer\n",
    "        self.batch_norm = nn.BatchNorm1d(256)  # Apply BatchNorm after fc1\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(256, 74)  # Output layer (num_classes will be 74)\n",
    "\n",
    "        # Step 3: Create a list of all parameters\n",
    "        self.params = list(self.base_model.named_parameters())\n",
    "        # print(f\"Total parameters: {len(self.params)}\")\n",
    "\n",
    "        # Identify the last 10 layers\n",
    "        self.last_15_params = self.params[-20:]  # Get the last 10 parameters\n",
    "\n",
    "        # Step 4: Freeze all layers except the last 10 layers\n",
    "        for name, param in self.base_model.named_parameters():\n",
    "            param.requires_grad = False  # Freeze all layers initially\n",
    "\n",
    "        # Step 5: Unfreeze the last 10 layers\n",
    "        for name, param in self.last_15_params:\n",
    "            param.requires_grad = True  # Unfreeze the last 10 layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through ResNet50 base model\n",
    "        x = self.base_model(x)  # Get features from ResNet50\n",
    "        x = self.fc1(x)          # Apply fully connected layer 1\n",
    "        x = self.batch_norm(x)   # Apply batch normalization after fc1\n",
    "        x = self.relu(x)         # ReLU activation\n",
    "        x = self.dropout(x)      # Apply dropout\n",
    "        x = self.fc2(x)          # Output layer\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = ResN50_20()\n",
    "\n",
    "# Assuming 'device' is the variable where the device is specified, e.g., 'cuda' or 'cpu'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the appropriate device (GPU if available, otherwise CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "# Display model summary\n",
    "summary(model, input_size=(3, 224, 224))  # Input size (C, H, W) for ResNet50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12c24a1-f2cf-4421-9baa-65afc5752e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b2332eb-2a09-4133-bfe8-1a53fd4d5f9f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 36\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "              ReLU-2         [-1, 64, 224, 224]               0\n",
      "            Conv2d-3         [-1, 64, 224, 224]          36,928\n",
      "              ReLU-4         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-5         [-1, 64, 112, 112]               0\n",
      "            Conv2d-6        [-1, 128, 112, 112]          73,856\n",
      "              ReLU-7        [-1, 128, 112, 112]               0\n",
      "            Conv2d-8        [-1, 128, 112, 112]         147,584\n",
      "              ReLU-9        [-1, 128, 112, 112]               0\n",
      "        MaxPool2d-10          [-1, 128, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]         295,168\n",
      "             ReLU-12          [-1, 256, 56, 56]               0\n",
      "           Conv2d-13          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-14          [-1, 256, 56, 56]               0\n",
      "           Conv2d-15          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-16          [-1, 256, 56, 56]               0\n",
      "           Conv2d-17          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-18          [-1, 256, 56, 56]               0\n",
      "        MaxPool2d-19          [-1, 256, 28, 28]               0\n",
      "           Conv2d-20          [-1, 512, 28, 28]       1,180,160\n",
      "             ReLU-21          [-1, 512, 28, 28]               0\n",
      "           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-23          [-1, 512, 28, 28]               0\n",
      "           Conv2d-24          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-25          [-1, 512, 28, 28]               0\n",
      "           Conv2d-26          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-27          [-1, 512, 28, 28]               0\n",
      "        MaxPool2d-28          [-1, 512, 14, 14]               0\n",
      "           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-30          [-1, 512, 14, 14]               0\n",
      "           Conv2d-31          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-32          [-1, 512, 14, 14]               0\n",
      "           Conv2d-33          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-34          [-1, 512, 14, 14]               0\n",
      "           Conv2d-35          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-36          [-1, 512, 14, 14]               0\n",
      "        MaxPool2d-37            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-38            [-1, 512, 7, 7]               0\n",
      "           Linear-39                 [-1, 4096]     102,764,544\n",
      "             ReLU-40                 [-1, 4096]               0\n",
      "          Dropout-41                 [-1, 4096]               0\n",
      "           Linear-42                 [-1, 4096]      16,781,312\n",
      "             ReLU-43                 [-1, 4096]               0\n",
      "          Dropout-44                 [-1, 4096]               0\n",
      "              VGG-45                 [-1, 4096]               0\n",
      "           Linear-46                  [-1, 256]       1,048,832\n",
      "      BatchNorm1d-47                  [-1, 256]             512\n",
      "             ReLU-48                  [-1, 256]               0\n",
      "          Dropout-49                  [-1, 256]               0\n",
      "           Linear-50                   [-1, 74]          19,018\n",
      "================================================================\n",
      "Total params: 140,638,602\n",
      "Trainable params: 17,849,674\n",
      "Non-trainable params: 122,788,928\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 238.72\n",
      "Params size (MB): 536.49\n",
      "Estimated Total Size (MB): 775.79\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Vgg19_5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Vgg19_5, self).__init__()\n",
    "\n",
    "        self.desc = \"[3,224,224] -> VGG19preTrained(-15:)[4096] -> 256 -> 74\"\n",
    "\n",
    "        # Step 1: Load VGG19 base model (without pre-trained weights)\n",
    "        self.base_model = models.vgg19(weights=None)  # Load VGG19 without pre-trained weights\n",
    "        self.base_model.classifier = nn.Sequential(*list(self.base_model.classifier.children())[:-1])  # Remove the final fully connected (fc) layer\n",
    "\n",
    "        # Step 2: Load the pre-trained weights into the base model\n",
    "        state_dict = torch.load(\"preTrained/vgg19_pretrained.pth\", weights_only=True)  # Path to custom pretrained weights\n",
    "        self.base_model.load_state_dict(state_dict, strict=False)  # Load the state_dict, assuming you have custom weights\n",
    "\n",
    "        # Custom layers after the base model\n",
    "        self.fc1 = nn.Linear(4096, 256)  # Custom fully connected layer for 4096 input features (VGG19 FC layer before output)\n",
    "        self.batch_norm = nn.BatchNorm1d(256)  # Apply BatchNorm after fc1\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(256, 74)  # Output layer (num_classes will be 74)\n",
    "\n",
    "        # Step 3: Create a list of all parameters\n",
    "        self.params = list(self.base_model.named_parameters())\n",
    "        print(f\"Total parameters: {len(self.params)}\")\n",
    "\n",
    "        # Identify the last 10 layers\n",
    "        self.last_5_params = self.params[-2:]  # Get the last 10 parameters\n",
    "\n",
    "        # Step 4: Freeze all layers except the last 10 layers\n",
    "        for name, param in self.base_model.named_parameters():\n",
    "            param.requires_grad = False  # Freeze all layers initially\n",
    "        # Step 5: Unfreeze the last 10 layers\n",
    "        for name, param in self.last_5_params:\n",
    "            param.requires_grad = True  # Unfreeze the last 10 layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through VGG19 base model\n",
    "        x = self.base_model(x)  # Get features from VGG19\n",
    "        x = self.fc1(x)         # Apply fully connected layer 1\n",
    "        x = self.batch_norm(x)  # Apply batch normalization after fc1\n",
    "        x = self.relu(x)        # ReLU activation\n",
    "        x = self.dropout(x)     # Apply dropout\n",
    "        x = self.fc2(x)         # Output layer\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "model = Vgg19_5()\n",
    "\n",
    "# Assuming 'device' is the variable where the device is specified, e.g., 'cuda' or 'cpu'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the appropriate device (GPU if available, otherwise CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "# Display model summary\n",
    "summary(model, input_size=(3, 224, 224))  # Input size (C, H, W) for VGG19\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3353f313-eb4c-49b4-a8af-ff222f3e0eae",
   "metadata": {},
   "source": [
    "Total params: 140,638,602\n",
    "Trainable params: 127,693,642\n",
    "Non-trainable params: 12,944,960\n",
    "----------------------------------------------------------------\n",
    "Input size (MB): 0.57\n",
    "Forward/backward pass size (MB): 238.72\n",
    "Params size (MB): 536.49\n",
    "Estimated Total Size (MB): 775.79\n",
    "\n",
    "===============================================================\n",
    "Total params: 140,638,602\n",
    "Trainable params: 132,413,258\n",
    "Non-trainable params: 8,225,344\n",
    "----------------------------------------------------------------\n",
    "Input size (MB): 0.57\n",
    "Forward/backward pass size (MB): 238.72\n",
    "Params size (MB): 536.49\n",
    "Estimated Total Size (MB): 775.79"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caf47fe7-f72c-427e-be85-84d0a823bbaa",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
      "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             ReLU-15          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
      "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "             ReLU-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
      "             ReLU-25          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
      "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
      "             ReLU-29           [-1, 64, 56, 56]               0\n",
      "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
      "             ReLU-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
      "             ReLU-35          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
      "             ReLU-39          [-1, 128, 56, 56]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "             ReLU-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-47          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-57          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
      "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
      "             ReLU-61          [-1, 128, 28, 28]               0\n",
      "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
      "             ReLU-64          [-1, 128, 28, 28]               0\n",
      "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-67          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
      "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
      "             ReLU-71          [-1, 128, 28, 28]               0\n",
      "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
      "             ReLU-74          [-1, 128, 28, 28]               0\n",
      "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-77          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
      "           Conv2d-79          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-80          [-1, 256, 28, 28]             512\n",
      "             ReLU-81          [-1, 256, 28, 28]               0\n",
      "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
      "             ReLU-84          [-1, 256, 14, 14]               0\n",
      "           Conv2d-85         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-87         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-89         [-1, 1024, 14, 14]               0\n",
      "       Bottleneck-90         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-91          [-1, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-92          [-1, 256, 14, 14]             512\n",
      "             ReLU-93          [-1, 256, 14, 14]               0\n",
      "           Conv2d-94          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-95          [-1, 256, 14, 14]             512\n",
      "             ReLU-96          [-1, 256, 14, 14]               0\n",
      "           Conv2d-97         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-99         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-100         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-101          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-102          [-1, 256, 14, 14]             512\n",
      "            ReLU-103          [-1, 256, 14, 14]               0\n",
      "          Conv2d-104          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-105          [-1, 256, 14, 14]             512\n",
      "            ReLU-106          [-1, 256, 14, 14]               0\n",
      "          Conv2d-107         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-109         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-110         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-111          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
      "            ReLU-113          [-1, 256, 14, 14]               0\n",
      "          Conv2d-114          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-115          [-1, 256, 14, 14]             512\n",
      "            ReLU-116          [-1, 256, 14, 14]               0\n",
      "          Conv2d-117         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-119         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-120         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-121          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-122          [-1, 256, 14, 14]             512\n",
      "            ReLU-123          [-1, 256, 14, 14]               0\n",
      "          Conv2d-124          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-125          [-1, 256, 14, 14]             512\n",
      "            ReLU-126          [-1, 256, 14, 14]               0\n",
      "          Conv2d-127         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-129         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
      "            ReLU-133          [-1, 256, 14, 14]               0\n",
      "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
      "            ReLU-136          [-1, 256, 14, 14]               0\n",
      "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-139         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-141          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-142          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-143          [-1, 512, 14, 14]               0\n",
      "          Conv2d-144            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-145            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-146            [-1, 512, 7, 7]               0\n",
      "          Conv2d-147           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-148           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-149           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-150           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-151           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-152           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-153            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-154            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-155            [-1, 512, 7, 7]               0\n",
      "          Conv2d-156            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-157            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-158            [-1, 512, 7, 7]               0\n",
      "          Conv2d-159           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-160           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-161           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-162           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-163            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-164            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-165            [-1, 512, 7, 7]               0\n",
      "          Conv2d-166            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-167            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-168            [-1, 512, 7, 7]               0\n",
      "          Conv2d-169           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-170           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-171           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-172           [-1, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
      "        Identity-174                 [-1, 2048]               0\n",
      "          ResNet-175                 [-1, 2048]               0\n",
      "          Linear-176                  [-1, 256]         524,544\n",
      "     BatchNorm1d-177                  [-1, 256]             512\n",
      "            ReLU-178                  [-1, 256]               0\n",
      "         Dropout-179                  [-1, 256]               0\n",
      "          Linear-180                   [-1, 74]          19,018\n",
      "================================================================\n",
      "Total params: 24,052,106\n",
      "Trainable params: 8,419,658\n",
      "Non-trainable params: 15,632,448\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 286.59\n",
      "Params size (MB): 91.75\n",
      "Estimated Total Size (MB): 378.92\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class ResN50_15(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResN50_15, self).__init__()\n",
    "\n",
    "        self.desc = \"[3,224,224] -> ResNet50preTrained(-15:)[2048] -> 256 -> 74\"\n",
    "\n",
    "        # Step 1: Load ResNet50 base model (without pre-trained weights)\n",
    "        self.base_model = models.resnet50(weights=None)  # No pretrained weights at first\n",
    "        self.base_model.fc = nn.Identity()  # Remove the final fully connected (fc) layer\n",
    "\n",
    "        # Step 2: Load the pre-trained weights into the base model\n",
    "        state_dict = torch.load(\"preTrained/resnet50_pretrained.pth\", weights_only=True)\n",
    "        self.base_model.load_state_dict(state_dict, strict=False)  # Load the pre-trained weights\n",
    "\n",
    "        # Custom layers after the base model\n",
    "        self.fc1 = nn.Linear(2048, 256)  # Fully connected layer\n",
    "        self.batch_norm = nn.BatchNorm1d(256)  # Apply BatchNorm after fc1\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(256, 74)  # Output layer (num_classes will be 74)\n",
    "\n",
    "        # Step 3: Create a list of all parameters\n",
    "        self.params = list(self.base_model.named_parameters())\n",
    "        # print(f\"Total parameters: {len(self.params)}\")\n",
    "\n",
    "        # Identify the last 10 layers\n",
    "        self.last_15_params = self.params[-15:]  # Get the last 10 parameters\n",
    "\n",
    "        # Step 4: Freeze all layers except the last 10 layers\n",
    "        for name, param in self.base_model.named_parameters():\n",
    "            param.requires_grad = False  # Freeze all layers initially\n",
    "\n",
    "        # Step 5: Unfreeze the last 10 layers\n",
    "        for name, param in self.last_15_params:\n",
    "            param.requires_grad = True  # Unfreeze the last 10 layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through ResNet50 base model\n",
    "        x = self.base_model(x)  # Get features from ResNet50\n",
    "        x = self.fc1(x)          # Apply fully connected layer 1\n",
    "        x = self.batch_norm(x)   # Apply batch normalization after fc1\n",
    "        x = self.relu(x)         # ReLU activation\n",
    "        x = self.dropout(x)      # Apply dropout\n",
    "        x = self.fc2(x)          # Output layer\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = ResN50_15()\n",
    "\n",
    "# Assuming 'device' is the variable where the device is specified, e.g., 'cuda' or 'cpu'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the appropriate device (GPU if available, otherwise CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "# Display model summary\n",
    "summary(model, input_size=(3, 224, 224))  # Input size (C, H, W) for ResNet50\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a9712fa-a3f8-4e3d-9161-1061919cab15",
   "metadata": {},
   "source": [
    "Total params: 24,052,106\n",
    "Trainable params: 9,473,354\n",
    "Non-trainable params: 14,578,752\n",
    "------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7fbcb354-1c1a-4179-93ff-ba34f32203bf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 40, 112, 112]           1,080\n",
      "       BatchNorm2d-2         [-1, 40, 112, 112]              80\n",
      "              SiLU-3         [-1, 40, 112, 112]               0\n",
      "            Conv2d-4         [-1, 40, 112, 112]             360\n",
      "       BatchNorm2d-5         [-1, 40, 112, 112]              80\n",
      "              SiLU-6         [-1, 40, 112, 112]               0\n",
      " AdaptiveAvgPool2d-7             [-1, 40, 1, 1]               0\n",
      "            Conv2d-8             [-1, 10, 1, 1]             410\n",
      "              SiLU-9             [-1, 10, 1, 1]               0\n",
      "           Conv2d-10             [-1, 40, 1, 1]             440\n",
      "          Sigmoid-11             [-1, 40, 1, 1]               0\n",
      "SqueezeExcitation-12         [-1, 40, 112, 112]               0\n",
      "           Conv2d-13         [-1, 24, 112, 112]             960\n",
      "      BatchNorm2d-14         [-1, 24, 112, 112]              48\n",
      "           MBConv-15         [-1, 24, 112, 112]               0\n",
      "           Conv2d-16         [-1, 24, 112, 112]             216\n",
      "      BatchNorm2d-17         [-1, 24, 112, 112]              48\n",
      "             SiLU-18         [-1, 24, 112, 112]               0\n",
      "AdaptiveAvgPool2d-19             [-1, 24, 1, 1]               0\n",
      "           Conv2d-20              [-1, 6, 1, 1]             150\n",
      "             SiLU-21              [-1, 6, 1, 1]               0\n",
      "           Conv2d-22             [-1, 24, 1, 1]             168\n",
      "          Sigmoid-23             [-1, 24, 1, 1]               0\n",
      "SqueezeExcitation-24         [-1, 24, 112, 112]               0\n",
      "           Conv2d-25         [-1, 24, 112, 112]             576\n",
      "      BatchNorm2d-26         [-1, 24, 112, 112]              48\n",
      "  StochasticDepth-27         [-1, 24, 112, 112]               0\n",
      "           MBConv-28         [-1, 24, 112, 112]               0\n",
      "           Conv2d-29        [-1, 144, 112, 112]           3,456\n",
      "      BatchNorm2d-30        [-1, 144, 112, 112]             288\n",
      "             SiLU-31        [-1, 144, 112, 112]               0\n",
      "           Conv2d-32          [-1, 144, 56, 56]           1,296\n",
      "      BatchNorm2d-33          [-1, 144, 56, 56]             288\n",
      "             SiLU-34          [-1, 144, 56, 56]               0\n",
      "AdaptiveAvgPool2d-35            [-1, 144, 1, 1]               0\n",
      "           Conv2d-36              [-1, 6, 1, 1]             870\n",
      "             SiLU-37              [-1, 6, 1, 1]               0\n",
      "           Conv2d-38            [-1, 144, 1, 1]           1,008\n",
      "          Sigmoid-39            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-40          [-1, 144, 56, 56]               0\n",
      "           Conv2d-41           [-1, 32, 56, 56]           4,608\n",
      "      BatchNorm2d-42           [-1, 32, 56, 56]              64\n",
      "           MBConv-43           [-1, 32, 56, 56]               0\n",
      "           Conv2d-44          [-1, 192, 56, 56]           6,144\n",
      "      BatchNorm2d-45          [-1, 192, 56, 56]             384\n",
      "             SiLU-46          [-1, 192, 56, 56]               0\n",
      "           Conv2d-47          [-1, 192, 56, 56]           1,728\n",
      "      BatchNorm2d-48          [-1, 192, 56, 56]             384\n",
      "             SiLU-49          [-1, 192, 56, 56]               0\n",
      "AdaptiveAvgPool2d-50            [-1, 192, 1, 1]               0\n",
      "           Conv2d-51              [-1, 8, 1, 1]           1,544\n",
      "             SiLU-52              [-1, 8, 1, 1]               0\n",
      "           Conv2d-53            [-1, 192, 1, 1]           1,728\n",
      "          Sigmoid-54            [-1, 192, 1, 1]               0\n",
      "SqueezeExcitation-55          [-1, 192, 56, 56]               0\n",
      "           Conv2d-56           [-1, 32, 56, 56]           6,144\n",
      "      BatchNorm2d-57           [-1, 32, 56, 56]              64\n",
      "  StochasticDepth-58           [-1, 32, 56, 56]               0\n",
      "           MBConv-59           [-1, 32, 56, 56]               0\n",
      "           Conv2d-60          [-1, 192, 56, 56]           6,144\n",
      "      BatchNorm2d-61          [-1, 192, 56, 56]             384\n",
      "             SiLU-62          [-1, 192, 56, 56]               0\n",
      "           Conv2d-63          [-1, 192, 56, 56]           1,728\n",
      "      BatchNorm2d-64          [-1, 192, 56, 56]             384\n",
      "             SiLU-65          [-1, 192, 56, 56]               0\n",
      "AdaptiveAvgPool2d-66            [-1, 192, 1, 1]               0\n",
      "           Conv2d-67              [-1, 8, 1, 1]           1,544\n",
      "             SiLU-68              [-1, 8, 1, 1]               0\n",
      "           Conv2d-69            [-1, 192, 1, 1]           1,728\n",
      "          Sigmoid-70            [-1, 192, 1, 1]               0\n",
      "SqueezeExcitation-71          [-1, 192, 56, 56]               0\n",
      "           Conv2d-72           [-1, 32, 56, 56]           6,144\n",
      "      BatchNorm2d-73           [-1, 32, 56, 56]              64\n",
      "  StochasticDepth-74           [-1, 32, 56, 56]               0\n",
      "           MBConv-75           [-1, 32, 56, 56]               0\n",
      "           Conv2d-76          [-1, 192, 56, 56]           6,144\n",
      "      BatchNorm2d-77          [-1, 192, 56, 56]             384\n",
      "             SiLU-78          [-1, 192, 56, 56]               0\n",
      "           Conv2d-79          [-1, 192, 28, 28]           4,800\n",
      "      BatchNorm2d-80          [-1, 192, 28, 28]             384\n",
      "             SiLU-81          [-1, 192, 28, 28]               0\n",
      "AdaptiveAvgPool2d-82            [-1, 192, 1, 1]               0\n",
      "           Conv2d-83              [-1, 8, 1, 1]           1,544\n",
      "             SiLU-84              [-1, 8, 1, 1]               0\n",
      "           Conv2d-85            [-1, 192, 1, 1]           1,728\n",
      "          Sigmoid-86            [-1, 192, 1, 1]               0\n",
      "SqueezeExcitation-87          [-1, 192, 28, 28]               0\n",
      "           Conv2d-88           [-1, 48, 28, 28]           9,216\n",
      "      BatchNorm2d-89           [-1, 48, 28, 28]              96\n",
      "           MBConv-90           [-1, 48, 28, 28]               0\n",
      "           Conv2d-91          [-1, 288, 28, 28]          13,824\n",
      "      BatchNorm2d-92          [-1, 288, 28, 28]             576\n",
      "             SiLU-93          [-1, 288, 28, 28]               0\n",
      "           Conv2d-94          [-1, 288, 28, 28]           7,200\n",
      "      BatchNorm2d-95          [-1, 288, 28, 28]             576\n",
      "             SiLU-96          [-1, 288, 28, 28]               0\n",
      "AdaptiveAvgPool2d-97            [-1, 288, 1, 1]               0\n",
      "           Conv2d-98             [-1, 12, 1, 1]           3,468\n",
      "             SiLU-99             [-1, 12, 1, 1]               0\n",
      "          Conv2d-100            [-1, 288, 1, 1]           3,744\n",
      "         Sigmoid-101            [-1, 288, 1, 1]               0\n",
      "SqueezeExcitation-102          [-1, 288, 28, 28]               0\n",
      "          Conv2d-103           [-1, 48, 28, 28]          13,824\n",
      "     BatchNorm2d-104           [-1, 48, 28, 28]              96\n",
      " StochasticDepth-105           [-1, 48, 28, 28]               0\n",
      "          MBConv-106           [-1, 48, 28, 28]               0\n",
      "          Conv2d-107          [-1, 288, 28, 28]          13,824\n",
      "     BatchNorm2d-108          [-1, 288, 28, 28]             576\n",
      "            SiLU-109          [-1, 288, 28, 28]               0\n",
      "          Conv2d-110          [-1, 288, 28, 28]           7,200\n",
      "     BatchNorm2d-111          [-1, 288, 28, 28]             576\n",
      "            SiLU-112          [-1, 288, 28, 28]               0\n",
      "AdaptiveAvgPool2d-113            [-1, 288, 1, 1]               0\n",
      "          Conv2d-114             [-1, 12, 1, 1]           3,468\n",
      "            SiLU-115             [-1, 12, 1, 1]               0\n",
      "          Conv2d-116            [-1, 288, 1, 1]           3,744\n",
      "         Sigmoid-117            [-1, 288, 1, 1]               0\n",
      "SqueezeExcitation-118          [-1, 288, 28, 28]               0\n",
      "          Conv2d-119           [-1, 48, 28, 28]          13,824\n",
      "     BatchNorm2d-120           [-1, 48, 28, 28]              96\n",
      " StochasticDepth-121           [-1, 48, 28, 28]               0\n",
      "          MBConv-122           [-1, 48, 28, 28]               0\n",
      "          Conv2d-123          [-1, 288, 28, 28]          13,824\n",
      "     BatchNorm2d-124          [-1, 288, 28, 28]             576\n",
      "            SiLU-125          [-1, 288, 28, 28]               0\n",
      "          Conv2d-126          [-1, 288, 14, 14]           2,592\n",
      "     BatchNorm2d-127          [-1, 288, 14, 14]             576\n",
      "            SiLU-128          [-1, 288, 14, 14]               0\n",
      "AdaptiveAvgPool2d-129            [-1, 288, 1, 1]               0\n",
      "          Conv2d-130             [-1, 12, 1, 1]           3,468\n",
      "            SiLU-131             [-1, 12, 1, 1]               0\n",
      "          Conv2d-132            [-1, 288, 1, 1]           3,744\n",
      "         Sigmoid-133            [-1, 288, 1, 1]               0\n",
      "SqueezeExcitation-134          [-1, 288, 14, 14]               0\n",
      "          Conv2d-135           [-1, 96, 14, 14]          27,648\n",
      "     BatchNorm2d-136           [-1, 96, 14, 14]             192\n",
      "          MBConv-137           [-1, 96, 14, 14]               0\n",
      "          Conv2d-138          [-1, 576, 14, 14]          55,296\n",
      "     BatchNorm2d-139          [-1, 576, 14, 14]           1,152\n",
      "            SiLU-140          [-1, 576, 14, 14]               0\n",
      "          Conv2d-141          [-1, 576, 14, 14]           5,184\n",
      "     BatchNorm2d-142          [-1, 576, 14, 14]           1,152\n",
      "            SiLU-143          [-1, 576, 14, 14]               0\n",
      "AdaptiveAvgPool2d-144            [-1, 576, 1, 1]               0\n",
      "          Conv2d-145             [-1, 24, 1, 1]          13,848\n",
      "            SiLU-146             [-1, 24, 1, 1]               0\n",
      "          Conv2d-147            [-1, 576, 1, 1]          14,400\n",
      "         Sigmoid-148            [-1, 576, 1, 1]               0\n",
      "SqueezeExcitation-149          [-1, 576, 14, 14]               0\n",
      "          Conv2d-150           [-1, 96, 14, 14]          55,296\n",
      "     BatchNorm2d-151           [-1, 96, 14, 14]             192\n",
      " StochasticDepth-152           [-1, 96, 14, 14]               0\n",
      "          MBConv-153           [-1, 96, 14, 14]               0\n",
      "          Conv2d-154          [-1, 576, 14, 14]          55,296\n",
      "     BatchNorm2d-155          [-1, 576, 14, 14]           1,152\n",
      "            SiLU-156          [-1, 576, 14, 14]               0\n",
      "          Conv2d-157          [-1, 576, 14, 14]           5,184\n",
      "     BatchNorm2d-158          [-1, 576, 14, 14]           1,152\n",
      "            SiLU-159          [-1, 576, 14, 14]               0\n",
      "AdaptiveAvgPool2d-160            [-1, 576, 1, 1]               0\n",
      "          Conv2d-161             [-1, 24, 1, 1]          13,848\n",
      "            SiLU-162             [-1, 24, 1, 1]               0\n",
      "          Conv2d-163            [-1, 576, 1, 1]          14,400\n",
      "         Sigmoid-164            [-1, 576, 1, 1]               0\n",
      "SqueezeExcitation-165          [-1, 576, 14, 14]               0\n",
      "          Conv2d-166           [-1, 96, 14, 14]          55,296\n",
      "     BatchNorm2d-167           [-1, 96, 14, 14]             192\n",
      " StochasticDepth-168           [-1, 96, 14, 14]               0\n",
      "          MBConv-169           [-1, 96, 14, 14]               0\n",
      "          Conv2d-170          [-1, 576, 14, 14]          55,296\n",
      "     BatchNorm2d-171          [-1, 576, 14, 14]           1,152\n",
      "            SiLU-172          [-1, 576, 14, 14]               0\n",
      "          Conv2d-173          [-1, 576, 14, 14]           5,184\n",
      "     BatchNorm2d-174          [-1, 576, 14, 14]           1,152\n",
      "            SiLU-175          [-1, 576, 14, 14]               0\n",
      "AdaptiveAvgPool2d-176            [-1, 576, 1, 1]               0\n",
      "          Conv2d-177             [-1, 24, 1, 1]          13,848\n",
      "            SiLU-178             [-1, 24, 1, 1]               0\n",
      "          Conv2d-179            [-1, 576, 1, 1]          14,400\n",
      "         Sigmoid-180            [-1, 576, 1, 1]               0\n",
      "SqueezeExcitation-181          [-1, 576, 14, 14]               0\n",
      "          Conv2d-182           [-1, 96, 14, 14]          55,296\n",
      "     BatchNorm2d-183           [-1, 96, 14, 14]             192\n",
      " StochasticDepth-184           [-1, 96, 14, 14]               0\n",
      "          MBConv-185           [-1, 96, 14, 14]               0\n",
      "          Conv2d-186          [-1, 576, 14, 14]          55,296\n",
      "     BatchNorm2d-187          [-1, 576, 14, 14]           1,152\n",
      "            SiLU-188          [-1, 576, 14, 14]               0\n",
      "          Conv2d-189          [-1, 576, 14, 14]           5,184\n",
      "     BatchNorm2d-190          [-1, 576, 14, 14]           1,152\n",
      "            SiLU-191          [-1, 576, 14, 14]               0\n",
      "AdaptiveAvgPool2d-192            [-1, 576, 1, 1]               0\n",
      "          Conv2d-193             [-1, 24, 1, 1]          13,848\n",
      "            SiLU-194             [-1, 24, 1, 1]               0\n",
      "          Conv2d-195            [-1, 576, 1, 1]          14,400\n",
      "         Sigmoid-196            [-1, 576, 1, 1]               0\n",
      "SqueezeExcitation-197          [-1, 576, 14, 14]               0\n",
      "          Conv2d-198           [-1, 96, 14, 14]          55,296\n",
      "     BatchNorm2d-199           [-1, 96, 14, 14]             192\n",
      " StochasticDepth-200           [-1, 96, 14, 14]               0\n",
      "          MBConv-201           [-1, 96, 14, 14]               0\n",
      "          Conv2d-202          [-1, 576, 14, 14]          55,296\n",
      "     BatchNorm2d-203          [-1, 576, 14, 14]           1,152\n",
      "            SiLU-204          [-1, 576, 14, 14]               0\n",
      "          Conv2d-205          [-1, 576, 14, 14]          14,400\n",
      "     BatchNorm2d-206          [-1, 576, 14, 14]           1,152\n",
      "            SiLU-207          [-1, 576, 14, 14]               0\n",
      "AdaptiveAvgPool2d-208            [-1, 576, 1, 1]               0\n",
      "          Conv2d-209             [-1, 24, 1, 1]          13,848\n",
      "            SiLU-210             [-1, 24, 1, 1]               0\n",
      "          Conv2d-211            [-1, 576, 1, 1]          14,400\n",
      "         Sigmoid-212            [-1, 576, 1, 1]               0\n",
      "SqueezeExcitation-213          [-1, 576, 14, 14]               0\n",
      "          Conv2d-214          [-1, 136, 14, 14]          78,336\n",
      "     BatchNorm2d-215          [-1, 136, 14, 14]             272\n",
      "          MBConv-216          [-1, 136, 14, 14]               0\n",
      "          Conv2d-217          [-1, 816, 14, 14]         110,976\n",
      "     BatchNorm2d-218          [-1, 816, 14, 14]           1,632\n",
      "            SiLU-219          [-1, 816, 14, 14]               0\n",
      "          Conv2d-220          [-1, 816, 14, 14]          20,400\n",
      "     BatchNorm2d-221          [-1, 816, 14, 14]           1,632\n",
      "            SiLU-222          [-1, 816, 14, 14]               0\n",
      "AdaptiveAvgPool2d-223            [-1, 816, 1, 1]               0\n",
      "          Conv2d-224             [-1, 34, 1, 1]          27,778\n",
      "            SiLU-225             [-1, 34, 1, 1]               0\n",
      "          Conv2d-226            [-1, 816, 1, 1]          28,560\n",
      "         Sigmoid-227            [-1, 816, 1, 1]               0\n",
      "SqueezeExcitation-228          [-1, 816, 14, 14]               0\n",
      "          Conv2d-229          [-1, 136, 14, 14]         110,976\n",
      "     BatchNorm2d-230          [-1, 136, 14, 14]             272\n",
      " StochasticDepth-231          [-1, 136, 14, 14]               0\n",
      "          MBConv-232          [-1, 136, 14, 14]               0\n",
      "          Conv2d-233          [-1, 816, 14, 14]         110,976\n",
      "     BatchNorm2d-234          [-1, 816, 14, 14]           1,632\n",
      "            SiLU-235          [-1, 816, 14, 14]               0\n",
      "          Conv2d-236          [-1, 816, 14, 14]          20,400\n",
      "     BatchNorm2d-237          [-1, 816, 14, 14]           1,632\n",
      "            SiLU-238          [-1, 816, 14, 14]               0\n",
      "AdaptiveAvgPool2d-239            [-1, 816, 1, 1]               0\n",
      "          Conv2d-240             [-1, 34, 1, 1]          27,778\n",
      "            SiLU-241             [-1, 34, 1, 1]               0\n",
      "          Conv2d-242            [-1, 816, 1, 1]          28,560\n",
      "         Sigmoid-243            [-1, 816, 1, 1]               0\n",
      "SqueezeExcitation-244          [-1, 816, 14, 14]               0\n",
      "          Conv2d-245          [-1, 136, 14, 14]         110,976\n",
      "     BatchNorm2d-246          [-1, 136, 14, 14]             272\n",
      " StochasticDepth-247          [-1, 136, 14, 14]               0\n",
      "          MBConv-248          [-1, 136, 14, 14]               0\n",
      "          Conv2d-249          [-1, 816, 14, 14]         110,976\n",
      "     BatchNorm2d-250          [-1, 816, 14, 14]           1,632\n",
      "            SiLU-251          [-1, 816, 14, 14]               0\n",
      "          Conv2d-252          [-1, 816, 14, 14]          20,400\n",
      "     BatchNorm2d-253          [-1, 816, 14, 14]           1,632\n",
      "            SiLU-254          [-1, 816, 14, 14]               0\n",
      "AdaptiveAvgPool2d-255            [-1, 816, 1, 1]               0\n",
      "          Conv2d-256             [-1, 34, 1, 1]          27,778\n",
      "            SiLU-257             [-1, 34, 1, 1]               0\n",
      "          Conv2d-258            [-1, 816, 1, 1]          28,560\n",
      "         Sigmoid-259            [-1, 816, 1, 1]               0\n",
      "SqueezeExcitation-260          [-1, 816, 14, 14]               0\n",
      "          Conv2d-261          [-1, 136, 14, 14]         110,976\n",
      "     BatchNorm2d-262          [-1, 136, 14, 14]             272\n",
      " StochasticDepth-263          [-1, 136, 14, 14]               0\n",
      "          MBConv-264          [-1, 136, 14, 14]               0\n",
      "          Conv2d-265          [-1, 816, 14, 14]         110,976\n",
      "     BatchNorm2d-266          [-1, 816, 14, 14]           1,632\n",
      "            SiLU-267          [-1, 816, 14, 14]               0\n",
      "          Conv2d-268          [-1, 816, 14, 14]          20,400\n",
      "     BatchNorm2d-269          [-1, 816, 14, 14]           1,632\n",
      "            SiLU-270          [-1, 816, 14, 14]               0\n",
      "AdaptiveAvgPool2d-271            [-1, 816, 1, 1]               0\n",
      "          Conv2d-272             [-1, 34, 1, 1]          27,778\n",
      "            SiLU-273             [-1, 34, 1, 1]               0\n",
      "          Conv2d-274            [-1, 816, 1, 1]          28,560\n",
      "         Sigmoid-275            [-1, 816, 1, 1]               0\n",
      "SqueezeExcitation-276          [-1, 816, 14, 14]               0\n",
      "          Conv2d-277          [-1, 136, 14, 14]         110,976\n",
      "     BatchNorm2d-278          [-1, 136, 14, 14]             272\n",
      " StochasticDepth-279          [-1, 136, 14, 14]               0\n",
      "          MBConv-280          [-1, 136, 14, 14]               0\n",
      "          Conv2d-281          [-1, 816, 14, 14]         110,976\n",
      "     BatchNorm2d-282          [-1, 816, 14, 14]           1,632\n",
      "            SiLU-283          [-1, 816, 14, 14]               0\n",
      "          Conv2d-284            [-1, 816, 7, 7]          20,400\n",
      "     BatchNorm2d-285            [-1, 816, 7, 7]           1,632\n",
      "            SiLU-286            [-1, 816, 7, 7]               0\n",
      "AdaptiveAvgPool2d-287            [-1, 816, 1, 1]               0\n",
      "          Conv2d-288             [-1, 34, 1, 1]          27,778\n",
      "            SiLU-289             [-1, 34, 1, 1]               0\n",
      "          Conv2d-290            [-1, 816, 1, 1]          28,560\n",
      "         Sigmoid-291            [-1, 816, 1, 1]               0\n",
      "SqueezeExcitation-292            [-1, 816, 7, 7]               0\n",
      "          Conv2d-293            [-1, 232, 7, 7]         189,312\n",
      "     BatchNorm2d-294            [-1, 232, 7, 7]             464\n",
      "          MBConv-295            [-1, 232, 7, 7]               0\n",
      "          Conv2d-296           [-1, 1392, 7, 7]         322,944\n",
      "     BatchNorm2d-297           [-1, 1392, 7, 7]           2,784\n",
      "            SiLU-298           [-1, 1392, 7, 7]               0\n",
      "          Conv2d-299           [-1, 1392, 7, 7]          34,800\n",
      "     BatchNorm2d-300           [-1, 1392, 7, 7]           2,784\n",
      "            SiLU-301           [-1, 1392, 7, 7]               0\n",
      "AdaptiveAvgPool2d-302           [-1, 1392, 1, 1]               0\n",
      "          Conv2d-303             [-1, 58, 1, 1]          80,794\n",
      "            SiLU-304             [-1, 58, 1, 1]               0\n",
      "          Conv2d-305           [-1, 1392, 1, 1]          82,128\n",
      "         Sigmoid-306           [-1, 1392, 1, 1]               0\n",
      "SqueezeExcitation-307           [-1, 1392, 7, 7]               0\n",
      "          Conv2d-308            [-1, 232, 7, 7]         322,944\n",
      "     BatchNorm2d-309            [-1, 232, 7, 7]             464\n",
      " StochasticDepth-310            [-1, 232, 7, 7]               0\n",
      "          MBConv-311            [-1, 232, 7, 7]               0\n",
      "          Conv2d-312           [-1, 1392, 7, 7]         322,944\n",
      "     BatchNorm2d-313           [-1, 1392, 7, 7]           2,784\n",
      "            SiLU-314           [-1, 1392, 7, 7]               0\n",
      "          Conv2d-315           [-1, 1392, 7, 7]          34,800\n",
      "     BatchNorm2d-316           [-1, 1392, 7, 7]           2,784\n",
      "            SiLU-317           [-1, 1392, 7, 7]               0\n",
      "AdaptiveAvgPool2d-318           [-1, 1392, 1, 1]               0\n",
      "          Conv2d-319             [-1, 58, 1, 1]          80,794\n",
      "            SiLU-320             [-1, 58, 1, 1]               0\n",
      "          Conv2d-321           [-1, 1392, 1, 1]          82,128\n",
      "         Sigmoid-322           [-1, 1392, 1, 1]               0\n",
      "SqueezeExcitation-323           [-1, 1392, 7, 7]               0\n",
      "          Conv2d-324            [-1, 232, 7, 7]         322,944\n",
      "     BatchNorm2d-325            [-1, 232, 7, 7]             464\n",
      " StochasticDepth-326            [-1, 232, 7, 7]               0\n",
      "          MBConv-327            [-1, 232, 7, 7]               0\n",
      "          Conv2d-328           [-1, 1392, 7, 7]         322,944\n",
      "     BatchNorm2d-329           [-1, 1392, 7, 7]           2,784\n",
      "            SiLU-330           [-1, 1392, 7, 7]               0\n",
      "          Conv2d-331           [-1, 1392, 7, 7]          34,800\n",
      "     BatchNorm2d-332           [-1, 1392, 7, 7]           2,784\n",
      "            SiLU-333           [-1, 1392, 7, 7]               0\n",
      "AdaptiveAvgPool2d-334           [-1, 1392, 1, 1]               0\n",
      "          Conv2d-335             [-1, 58, 1, 1]          80,794\n",
      "            SiLU-336             [-1, 58, 1, 1]               0\n",
      "          Conv2d-337           [-1, 1392, 1, 1]          82,128\n",
      "         Sigmoid-338           [-1, 1392, 1, 1]               0\n",
      "SqueezeExcitation-339           [-1, 1392, 7, 7]               0\n",
      "          Conv2d-340            [-1, 232, 7, 7]         322,944\n",
      "     BatchNorm2d-341            [-1, 232, 7, 7]             464\n",
      " StochasticDepth-342            [-1, 232, 7, 7]               0\n",
      "          MBConv-343            [-1, 232, 7, 7]               0\n",
      "          Conv2d-344           [-1, 1392, 7, 7]         322,944\n",
      "     BatchNorm2d-345           [-1, 1392, 7, 7]           2,784\n",
      "            SiLU-346           [-1, 1392, 7, 7]               0\n",
      "          Conv2d-347           [-1, 1392, 7, 7]          34,800\n",
      "     BatchNorm2d-348           [-1, 1392, 7, 7]           2,784\n",
      "            SiLU-349           [-1, 1392, 7, 7]               0\n",
      "AdaptiveAvgPool2d-350           [-1, 1392, 1, 1]               0\n",
      "          Conv2d-351             [-1, 58, 1, 1]          80,794\n",
      "            SiLU-352             [-1, 58, 1, 1]               0\n",
      "          Conv2d-353           [-1, 1392, 1, 1]          82,128\n",
      "         Sigmoid-354           [-1, 1392, 1, 1]               0\n",
      "SqueezeExcitation-355           [-1, 1392, 7, 7]               0\n",
      "          Conv2d-356            [-1, 232, 7, 7]         322,944\n",
      "     BatchNorm2d-357            [-1, 232, 7, 7]             464\n",
      " StochasticDepth-358            [-1, 232, 7, 7]               0\n",
      "          MBConv-359            [-1, 232, 7, 7]               0\n",
      "          Conv2d-360           [-1, 1392, 7, 7]         322,944\n",
      "     BatchNorm2d-361           [-1, 1392, 7, 7]           2,784\n",
      "            SiLU-362           [-1, 1392, 7, 7]               0\n",
      "          Conv2d-363           [-1, 1392, 7, 7]          34,800\n",
      "     BatchNorm2d-364           [-1, 1392, 7, 7]           2,784\n",
      "            SiLU-365           [-1, 1392, 7, 7]               0\n",
      "AdaptiveAvgPool2d-366           [-1, 1392, 1, 1]               0\n",
      "          Conv2d-367             [-1, 58, 1, 1]          80,794\n",
      "            SiLU-368             [-1, 58, 1, 1]               0\n",
      "          Conv2d-369           [-1, 1392, 1, 1]          82,128\n",
      "         Sigmoid-370           [-1, 1392, 1, 1]               0\n",
      "SqueezeExcitation-371           [-1, 1392, 7, 7]               0\n",
      "          Conv2d-372            [-1, 232, 7, 7]         322,944\n",
      "     BatchNorm2d-373            [-1, 232, 7, 7]             464\n",
      " StochasticDepth-374            [-1, 232, 7, 7]               0\n",
      "          MBConv-375            [-1, 232, 7, 7]               0\n",
      "          Conv2d-376           [-1, 1392, 7, 7]         322,944\n",
      "     BatchNorm2d-377           [-1, 1392, 7, 7]           2,784\n",
      "            SiLU-378           [-1, 1392, 7, 7]               0\n",
      "          Conv2d-379           [-1, 1392, 7, 7]          12,528\n",
      "     BatchNorm2d-380           [-1, 1392, 7, 7]           2,784\n",
      "            SiLU-381           [-1, 1392, 7, 7]               0\n",
      "AdaptiveAvgPool2d-382           [-1, 1392, 1, 1]               0\n",
      "          Conv2d-383             [-1, 58, 1, 1]          80,794\n",
      "            SiLU-384             [-1, 58, 1, 1]               0\n",
      "          Conv2d-385           [-1, 1392, 1, 1]          82,128\n",
      "         Sigmoid-386           [-1, 1392, 1, 1]               0\n",
      "SqueezeExcitation-387           [-1, 1392, 7, 7]               0\n",
      "          Conv2d-388            [-1, 384, 7, 7]         534,528\n",
      "     BatchNorm2d-389            [-1, 384, 7, 7]             768\n",
      "          MBConv-390            [-1, 384, 7, 7]               0\n",
      "          Conv2d-391           [-1, 2304, 7, 7]         884,736\n",
      "     BatchNorm2d-392           [-1, 2304, 7, 7]           4,608\n",
      "            SiLU-393           [-1, 2304, 7, 7]               0\n",
      "          Conv2d-394           [-1, 2304, 7, 7]          20,736\n",
      "     BatchNorm2d-395           [-1, 2304, 7, 7]           4,608\n",
      "            SiLU-396           [-1, 2304, 7, 7]               0\n",
      "AdaptiveAvgPool2d-397           [-1, 2304, 1, 1]               0\n",
      "          Conv2d-398             [-1, 96, 1, 1]         221,280\n",
      "            SiLU-399             [-1, 96, 1, 1]               0\n",
      "          Conv2d-400           [-1, 2304, 1, 1]         223,488\n",
      "         Sigmoid-401           [-1, 2304, 1, 1]               0\n",
      "SqueezeExcitation-402           [-1, 2304, 7, 7]               0\n",
      "          Conv2d-403            [-1, 384, 7, 7]         884,736\n",
      "     BatchNorm2d-404            [-1, 384, 7, 7]             768\n",
      " StochasticDepth-405            [-1, 384, 7, 7]               0\n",
      "          MBConv-406            [-1, 384, 7, 7]               0\n",
      "          Conv2d-407           [-1, 1536, 7, 7]         589,824\n",
      "     BatchNorm2d-408           [-1, 1536, 7, 7]           3,072\n",
      "            SiLU-409           [-1, 1536, 7, 7]               0\n",
      "AdaptiveAvgPool2d-410           [-1, 1536, 1, 1]               0\n",
      "        Identity-411                 [-1, 1536]               0\n",
      "    EfficientNet-412                 [-1, 1536]               0\n",
      "     BatchNorm1d-413                 [-1, 1536]           3,072\n",
      "          Linear-414                  [-1, 256]         393,472\n",
      "            ReLU-415                  [-1, 256]               0\n",
      "         Dropout-416                  [-1, 256]               0\n",
      "          Linear-417                   [-1, 74]          19,018\n",
      "================================================================\n",
      "Total params: 11,111,794\n",
      "Trainable params: 2,338,730\n",
      "Non-trainable params: 8,773,064\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 341.36\n",
      "Params size (MB): 42.39\n",
      "Estimated Total Size (MB): 384.32\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class EffiB3_10(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EffiB3_10, self).__init__()\n",
    "        self.desc = \"[3,224,224] ->EfficientNetB3preTrained(-10:)[1536] ->256 ->74\"\n",
    "        # Step 1: Load EfficientNet-B3 base model (without pre-trained weights)\n",
    "        self.base_model = models.efficientnet_b3(weights=None)  # No pretrained weights at first\n",
    "        self.base_model.classifier = nn.Identity()  # Remove final classification layer\n",
    "        \n",
    "        # Step 2: Load the pre-trained weights into the base model\n",
    "        state_dict = torch.load(\"preTrained/efficientnet_b3_weights.pth\", weights_only=True)\n",
    "        # Load the filtered state_dict into the model\n",
    "        self.base_model.load_state_dict(state_dict,strict=False)\n",
    "        # print(\"EfficientNet-B3 weights loaded successfully.\")\n",
    "        \n",
    "        # Custom layers added after the base model\n",
    "        self.batch_norm = nn.BatchNorm1d(1536)  # Output size of EfficientNet-B3 before classifier is 1536\n",
    "        self.fc1 = nn.Linear(1536, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(256, 74)\n",
    "        \n",
    "         # Step 3: Create a list of all parameters\n",
    "        self.params = list(self.base_model.named_parameters())\n",
    "        # print(f\"Total parameters: {len(self.params)}\")\n",
    "\n",
    "        # Identify the last 10 layers\n",
    "        self.last_10_params = self.params[-10:]  # Get the last 10 parameters\n",
    "\n",
    "        # Step 4: Freeze all layers except the last 10 layers\n",
    "        for name, param in self.base_model.named_parameters():\n",
    "            param.requires_grad = False  # Freeze all layers initially\n",
    "\n",
    "        # Step 5: Unfreeze the last 10 layers\n",
    "        for name, param in self.last_10_params:\n",
    "            param.requires_grad = True  # Unfreeze the last 10 layers\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward pass through EfficientNet-B3 base model\n",
    "        x = self.base_model(x)  # Get features from EfficientNet-B3\n",
    "        x = self.batch_norm(x)   # Apply batch normalization\n",
    "        x = self.fc1(x)          # Apply fully connected layer 1\n",
    "        x = self.relu(x)         # ReLU activation\n",
    "        x = self.dropout(x)      # Apply dropout\n",
    "        x = self.fc2(x)          # Output layer\n",
    "        return x\n",
    "\n",
    "# Initialize the model (freeze base model layers by default)\n",
    "model = EffiB3_10()\n",
    "# Assuming 'device' is the variable where the device is specified, e.g., 'cuda' or 'cpu'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the appropriate device (GPU if available, otherwise CPU)\n",
    "model = model.to(device)\n",
    "# Display model summary\n",
    "summary(model, input_size=(3, 224, 224))  # Input size (C, H, W) for EfficientNet-B3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1557ddf-0138-4485-842b-3915c863d7cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1b21aa8e-4ff0-4ff8-959a-dcf8687a4cc4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
      "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             ReLU-15          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
      "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "             ReLU-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
      "             ReLU-25          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
      "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
      "             ReLU-29           [-1, 64, 56, 56]               0\n",
      "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
      "             ReLU-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
      "             ReLU-35          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
      "             ReLU-39          [-1, 128, 56, 56]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "             ReLU-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-47          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-57          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
      "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
      "             ReLU-61          [-1, 128, 28, 28]               0\n",
      "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
      "             ReLU-64          [-1, 128, 28, 28]               0\n",
      "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-67          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
      "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
      "             ReLU-71          [-1, 128, 28, 28]               0\n",
      "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
      "             ReLU-74          [-1, 128, 28, 28]               0\n",
      "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-77          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
      "           Conv2d-79          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-80          [-1, 256, 28, 28]             512\n",
      "             ReLU-81          [-1, 256, 28, 28]               0\n",
      "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
      "             ReLU-84          [-1, 256, 14, 14]               0\n",
      "           Conv2d-85         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-87         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-89         [-1, 1024, 14, 14]               0\n",
      "       Bottleneck-90         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-91          [-1, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-92          [-1, 256, 14, 14]             512\n",
      "             ReLU-93          [-1, 256, 14, 14]               0\n",
      "           Conv2d-94          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-95          [-1, 256, 14, 14]             512\n",
      "             ReLU-96          [-1, 256, 14, 14]               0\n",
      "           Conv2d-97         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-99         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-100         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-101          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-102          [-1, 256, 14, 14]             512\n",
      "            ReLU-103          [-1, 256, 14, 14]               0\n",
      "          Conv2d-104          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-105          [-1, 256, 14, 14]             512\n",
      "            ReLU-106          [-1, 256, 14, 14]               0\n",
      "          Conv2d-107         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-109         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-110         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-111          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
      "            ReLU-113          [-1, 256, 14, 14]               0\n",
      "          Conv2d-114          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-115          [-1, 256, 14, 14]             512\n",
      "            ReLU-116          [-1, 256, 14, 14]               0\n",
      "          Conv2d-117         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-119         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-120         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-121          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-122          [-1, 256, 14, 14]             512\n",
      "            ReLU-123          [-1, 256, 14, 14]               0\n",
      "          Conv2d-124          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-125          [-1, 256, 14, 14]             512\n",
      "            ReLU-126          [-1, 256, 14, 14]               0\n",
      "          Conv2d-127         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-129         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
      "            ReLU-133          [-1, 256, 14, 14]               0\n",
      "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
      "            ReLU-136          [-1, 256, 14, 14]               0\n",
      "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-139         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-141          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-142          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-143          [-1, 512, 14, 14]               0\n",
      "          Conv2d-144            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-145            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-146            [-1, 512, 7, 7]               0\n",
      "          Conv2d-147           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-148           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-149           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-150           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-151           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-152           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-153            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-154            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-155            [-1, 512, 7, 7]               0\n",
      "          Conv2d-156            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-157            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-158            [-1, 512, 7, 7]               0\n",
      "          Conv2d-159           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-160           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-161           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-162           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-163            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-164            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-165            [-1, 512, 7, 7]               0\n",
      "          Conv2d-166            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-167            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-168            [-1, 512, 7, 7]               0\n",
      "          Conv2d-169           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-170           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-171           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-172           [-1, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
      "        Identity-174                 [-1, 2048]               0\n",
      "          ResNet-175                 [-1, 2048]               0\n",
      "          Linear-176                  [-1, 256]         524,544\n",
      "     BatchNorm1d-177                  [-1, 256]             512\n",
      "            ReLU-178                  [-1, 256]               0\n",
      "         Dropout-179                  [-1, 256]               0\n",
      "          Linear-180                   [-1, 74]          19,018\n",
      "================================================================\n",
      "Total params: 24,052,106\n",
      "Trainable params: 5,006,666\n",
      "Non-trainable params: 19,045,440\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 286.59\n",
      "Params size (MB): 91.75\n",
      "Estimated Total Size (MB): 378.92\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class ResN50_10(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResN50_10, self).__init__()\n",
    "\n",
    "        self.desc = \"[3,224,224] -> ResNet50preTrained[2048] -> 256 -> 74\"\n",
    "\n",
    "        # Step 1: Load ResNet50 base model (without pre-trained weights)\n",
    "        self.base_model = models.resnet50(weights=None)  # No pretrained weights at first\n",
    "        self.base_model.fc = nn.Identity()  # Remove the final fully connected (fc) layer\n",
    "\n",
    "        # Step 2: Load the pre-trained weights into the base model\n",
    "        state_dict = torch.load(\"preTrained/resnet50_pretrained.pth\", weights_only=True)\n",
    "        self.base_model.load_state_dict(state_dict, strict=False)  # Load the pre-trained weights\n",
    "\n",
    "        # Custom layers after the base model\n",
    "        self.fc1 = nn.Linear(2048, 256)  # Fully connected layer\n",
    "        self.batch_norm = nn.BatchNorm1d(256)  # Apply BatchNorm after fc1\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(256, 74)  # Output layer (num_classes will be 74)\n",
    "\n",
    "        # Step 3: Create a list of all parameters\n",
    "        self.params = list(self.base_model.named_parameters())\n",
    "        # print(f\"Total parameters: {len(self.params)}\")\n",
    "\n",
    "        # Identify the last 10 layers\n",
    "        self.last_10_params = self.params[-10:]  # Get the last 10 parameters\n",
    "\n",
    "        # Step 4: Freeze all layers except the last 10 layers\n",
    "        for name, param in self.base_model.named_parameters():\n",
    "            param.requires_grad = False  # Freeze all layers initially\n",
    "\n",
    "        # Step 5: Unfreeze the last 10 layers\n",
    "        for name, param in self.last_10_params:\n",
    "            param.requires_grad = True  # Unfreeze the last 10 layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through ResNet50 base model\n",
    "        x = self.base_model(x)  # Get features from ResNet50\n",
    "        x = self.fc1(x)          # Apply fully connected layer 1\n",
    "        x = self.batch_norm(x)   # Apply batch normalization after fc1\n",
    "        x = self.relu(x)         # ReLU activation\n",
    "        x = self.dropout(x)      # Apply dropout\n",
    "        x = self.fc2(x)          # Output layer\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = ResN50_10()\n",
    "\n",
    "# Assuming 'device' is the variable where the device is specified, e.g., 'cuda' or 'cpu'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the appropriate device (GPU if available, otherwise CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "# Display model summary\n",
    "summary(model, input_size=(3, 224, 224))  # Input size (C, H, W) for ResNet50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f9dfd375-a216-4fbc-867f-c7bc83127f7e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['conv1.weight',\n",
       "  'bn1.weight',\n",
       "  'bn1.bias',\n",
       "  'layer1.0.conv1.weight',\n",
       "  'layer1.0.bn1.weight',\n",
       "  'layer1.0.bn1.bias',\n",
       "  'layer1.0.conv2.weight',\n",
       "  'layer1.0.bn2.weight',\n",
       "  'layer1.0.bn2.bias',\n",
       "  'layer1.0.conv3.weight',\n",
       "  'layer1.0.bn3.weight',\n",
       "  'layer1.0.bn3.bias',\n",
       "  'layer1.0.downsample.0.weight',\n",
       "  'layer1.0.downsample.1.weight',\n",
       "  'layer1.0.downsample.1.bias',\n",
       "  'layer1.1.conv1.weight',\n",
       "  'layer1.1.bn1.weight',\n",
       "  'layer1.1.bn1.bias',\n",
       "  'layer1.1.conv2.weight',\n",
       "  'layer1.1.bn2.weight',\n",
       "  'layer1.1.bn2.bias',\n",
       "  'layer1.1.conv3.weight',\n",
       "  'layer1.1.bn3.weight',\n",
       "  'layer1.1.bn3.bias',\n",
       "  'layer1.2.conv1.weight',\n",
       "  'layer1.2.bn1.weight',\n",
       "  'layer1.2.bn1.bias',\n",
       "  'layer1.2.conv2.weight',\n",
       "  'layer1.2.bn2.weight',\n",
       "  'layer1.2.bn2.bias',\n",
       "  'layer1.2.conv3.weight',\n",
       "  'layer1.2.bn3.weight',\n",
       "  'layer1.2.bn3.bias',\n",
       "  'layer2.0.conv1.weight',\n",
       "  'layer2.0.bn1.weight',\n",
       "  'layer2.0.bn1.bias',\n",
       "  'layer2.0.conv2.weight',\n",
       "  'layer2.0.bn2.weight',\n",
       "  'layer2.0.bn2.bias',\n",
       "  'layer2.0.conv3.weight',\n",
       "  'layer2.0.bn3.weight',\n",
       "  'layer2.0.bn3.bias',\n",
       "  'layer2.0.downsample.0.weight',\n",
       "  'layer2.0.downsample.1.weight',\n",
       "  'layer2.0.downsample.1.bias',\n",
       "  'layer2.1.conv1.weight',\n",
       "  'layer2.1.bn1.weight',\n",
       "  'layer2.1.bn1.bias',\n",
       "  'layer2.1.conv2.weight',\n",
       "  'layer2.1.bn2.weight',\n",
       "  'layer2.1.bn2.bias',\n",
       "  'layer2.1.conv3.weight',\n",
       "  'layer2.1.bn3.weight',\n",
       "  'layer2.1.bn3.bias',\n",
       "  'layer2.2.conv1.weight',\n",
       "  'layer2.2.bn1.weight',\n",
       "  'layer2.2.bn1.bias',\n",
       "  'layer2.2.conv2.weight',\n",
       "  'layer2.2.bn2.weight',\n",
       "  'layer2.2.bn2.bias',\n",
       "  'layer2.2.conv3.weight',\n",
       "  'layer2.2.bn3.weight',\n",
       "  'layer2.2.bn3.bias',\n",
       "  'layer2.3.conv1.weight',\n",
       "  'layer2.3.bn1.weight',\n",
       "  'layer2.3.bn1.bias',\n",
       "  'layer2.3.conv2.weight',\n",
       "  'layer2.3.bn2.weight',\n",
       "  'layer2.3.bn2.bias',\n",
       "  'layer2.3.conv3.weight',\n",
       "  'layer2.3.bn3.weight',\n",
       "  'layer2.3.bn3.bias',\n",
       "  'layer3.0.conv1.weight',\n",
       "  'layer3.0.bn1.weight',\n",
       "  'layer3.0.bn1.bias',\n",
       "  'layer3.0.conv2.weight',\n",
       "  'layer3.0.bn2.weight',\n",
       "  'layer3.0.bn2.bias',\n",
       "  'layer3.0.conv3.weight',\n",
       "  'layer3.0.bn3.weight',\n",
       "  'layer3.0.bn3.bias',\n",
       "  'layer3.0.downsample.0.weight',\n",
       "  'layer3.0.downsample.1.weight',\n",
       "  'layer3.0.downsample.1.bias',\n",
       "  'layer3.1.conv1.weight',\n",
       "  'layer3.1.bn1.weight',\n",
       "  'layer3.1.bn1.bias',\n",
       "  'layer3.1.conv2.weight',\n",
       "  'layer3.1.bn2.weight',\n",
       "  'layer3.1.bn2.bias',\n",
       "  'layer3.1.conv3.weight',\n",
       "  'layer3.1.bn3.weight',\n",
       "  'layer3.1.bn3.bias',\n",
       "  'layer3.2.conv1.weight',\n",
       "  'layer3.2.bn1.weight',\n",
       "  'layer3.2.bn1.bias',\n",
       "  'layer3.2.conv2.weight',\n",
       "  'layer3.2.bn2.weight',\n",
       "  'layer3.2.bn2.bias',\n",
       "  'layer3.2.conv3.weight',\n",
       "  'layer3.2.bn3.weight',\n",
       "  'layer3.2.bn3.bias',\n",
       "  'layer3.3.conv1.weight',\n",
       "  'layer3.3.bn1.weight',\n",
       "  'layer3.3.bn1.bias',\n",
       "  'layer3.3.conv2.weight',\n",
       "  'layer3.3.bn2.weight',\n",
       "  'layer3.3.bn2.bias',\n",
       "  'layer3.3.conv3.weight',\n",
       "  'layer3.3.bn3.weight',\n",
       "  'layer3.3.bn3.bias',\n",
       "  'layer3.4.conv1.weight',\n",
       "  'layer3.4.bn1.weight',\n",
       "  'layer3.4.bn1.bias',\n",
       "  'layer3.4.conv2.weight',\n",
       "  'layer3.4.bn2.weight',\n",
       "  'layer3.4.bn2.bias',\n",
       "  'layer3.4.conv3.weight',\n",
       "  'layer3.4.bn3.weight',\n",
       "  'layer3.4.bn3.bias',\n",
       "  'layer3.5.conv1.weight',\n",
       "  'layer3.5.bn1.weight',\n",
       "  'layer3.5.bn1.bias',\n",
       "  'layer3.5.conv2.weight',\n",
       "  'layer3.5.bn2.weight',\n",
       "  'layer3.5.bn2.bias',\n",
       "  'layer3.5.conv3.weight',\n",
       "  'layer3.5.bn3.weight',\n",
       "  'layer3.5.bn3.bias',\n",
       "  'layer4.0.conv1.weight',\n",
       "  'layer4.0.bn1.weight',\n",
       "  'layer4.0.bn1.bias',\n",
       "  'layer4.0.conv2.weight',\n",
       "  'layer4.0.bn2.weight',\n",
       "  'layer4.0.bn2.bias',\n",
       "  'layer4.0.conv3.weight',\n",
       "  'layer4.0.bn3.weight',\n",
       "  'layer4.0.bn3.bias',\n",
       "  'layer4.0.downsample.0.weight',\n",
       "  'layer4.0.downsample.1.weight',\n",
       "  'layer4.0.downsample.1.bias',\n",
       "  'layer4.1.conv1.weight',\n",
       "  'layer4.1.bn1.weight',\n",
       "  'layer4.1.bn1.bias',\n",
       "  'layer4.1.conv2.weight',\n",
       "  'layer4.1.bn2.weight',\n",
       "  'layer4.1.bn2.bias',\n",
       "  'layer4.1.conv3.weight',\n",
       "  'layer4.1.bn3.weight',\n",
       "  'layer4.1.bn3.bias',\n",
       "  'layer4.2.conv1.weight',\n",
       "  'layer4.2.bn1.weight',\n",
       "  'layer4.2.bn1.bias',\n",
       "  'layer4.2.conv2.weight',\n",
       "  'layer4.2.bn2.weight',\n",
       "  'layer4.2.bn2.bias',\n",
       "  'layer4.2.conv3.weight',\n",
       "  'layer4.2.bn3.weight',\n",
       "  'layer4.2.bn3.bias'],\n",
       " 159)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = [name  for name,param  in model.base_model.named_parameters() ]\n",
    "# len(list(df.keys()))\n",
    "df,len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06cdab1-42b5-4306-bb1b-f55fbc87b96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(model.base_model.layer4.children()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47f9781-a5c7-49a4-81bf-d0c324551222",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d65816e6-035c-4705-b9ee-8a6ac9bcb093",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
      "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             ReLU-15          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
      "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "             ReLU-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
      "             ReLU-25          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
      "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
      "             ReLU-29           [-1, 64, 56, 56]               0\n",
      "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
      "             ReLU-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
      "             ReLU-35          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
      "             ReLU-39          [-1, 128, 56, 56]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "             ReLU-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-47          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-57          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
      "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
      "             ReLU-61          [-1, 128, 28, 28]               0\n",
      "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
      "             ReLU-64          [-1, 128, 28, 28]               0\n",
      "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-67          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
      "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
      "             ReLU-71          [-1, 128, 28, 28]               0\n",
      "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
      "             ReLU-74          [-1, 128, 28, 28]               0\n",
      "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-77          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
      "           Conv2d-79          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-80          [-1, 256, 28, 28]             512\n",
      "             ReLU-81          [-1, 256, 28, 28]               0\n",
      "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
      "             ReLU-84          [-1, 256, 14, 14]               0\n",
      "           Conv2d-85         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-87         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-89         [-1, 1024, 14, 14]               0\n",
      "       Bottleneck-90         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-91          [-1, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-92          [-1, 256, 14, 14]             512\n",
      "             ReLU-93          [-1, 256, 14, 14]               0\n",
      "           Conv2d-94          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-95          [-1, 256, 14, 14]             512\n",
      "             ReLU-96          [-1, 256, 14, 14]               0\n",
      "           Conv2d-97         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-99         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-100         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-101          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-102          [-1, 256, 14, 14]             512\n",
      "            ReLU-103          [-1, 256, 14, 14]               0\n",
      "          Conv2d-104          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-105          [-1, 256, 14, 14]             512\n",
      "            ReLU-106          [-1, 256, 14, 14]               0\n",
      "          Conv2d-107         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-109         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-110         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-111          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
      "            ReLU-113          [-1, 256, 14, 14]               0\n",
      "          Conv2d-114          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-115          [-1, 256, 14, 14]             512\n",
      "            ReLU-116          [-1, 256, 14, 14]               0\n",
      "          Conv2d-117         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-119         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-120         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-121          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-122          [-1, 256, 14, 14]             512\n",
      "            ReLU-123          [-1, 256, 14, 14]               0\n",
      "          Conv2d-124          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-125          [-1, 256, 14, 14]             512\n",
      "            ReLU-126          [-1, 256, 14, 14]               0\n",
      "          Conv2d-127         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-129         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
      "            ReLU-133          [-1, 256, 14, 14]               0\n",
      "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
      "            ReLU-136          [-1, 256, 14, 14]               0\n",
      "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-139         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-141          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-142          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-143          [-1, 512, 14, 14]               0\n",
      "          Conv2d-144            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-145            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-146            [-1, 512, 7, 7]               0\n",
      "          Conv2d-147           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-148           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-149           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-150           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-151           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-152           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-153            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-154            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-155            [-1, 512, 7, 7]               0\n",
      "          Conv2d-156            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-157            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-158            [-1, 512, 7, 7]               0\n",
      "          Conv2d-159           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-160           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-161           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-162           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-163            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-164            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-165            [-1, 512, 7, 7]               0\n",
      "          Conv2d-166            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-167            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-168            [-1, 512, 7, 7]               0\n",
      "          Conv2d-169           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-170           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-171           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-172           [-1, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
      "        Identity-174                 [-1, 2048]               0\n",
      "          ResNet-175                 [-1, 2048]               0\n",
      "          Linear-176                  [-1, 256]         524,544\n",
      "     BatchNorm1d-177                  [-1, 256]             512\n",
      "            ReLU-178                  [-1, 256]               0\n",
      "         Dropout-179                  [-1, 256]               0\n",
      "          Linear-180                   [-1, 74]          19,018\n",
      "================================================================\n",
      "Total params: 24,052,106\n",
      "Trainable params: 544,074\n",
      "Non-trainable params: 23,508,032\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 286.59\n",
      "Params size (MB): 91.75\n",
      "Estimated Total Size (MB): 378.92\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class ResN50(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResN50, self).__init__()\n",
    "\n",
    "        self.desc = \"[3,224,224] -> ResNet50preTrained[2048] -> 256 -> 74\"\n",
    "\n",
    "        # Step 1: Load ResNet50 base model (without pre-trained weights)\n",
    "        self.base_model = models.resnet50(weights=None)  # No pretrained weights at first\n",
    "        self.base_model.fc = nn.Identity()  # Remove the final fully connected (fc) layer\n",
    "\n",
    "        # Step 2: Load the pre-trained weights into the base model\n",
    "        state_dict = torch.load(\"preTrained/resnet50_pretrained.pth\", weights_only=True)\n",
    "        self.base_model.load_state_dict(state_dict, strict=False)  # Uncomment if you have custom pretrained weights\n",
    "\n",
    "        # Custom layers after the base model\n",
    "        self.fc1 = nn.Linear(2048, 256)  # Updated: Custom fully connected layer for 2048 input features\n",
    "        self.batch_norm = nn.BatchNorm1d(256)  # Apply BatchNorm after fc1\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(256, 74)  # Output layer (num_classes will be 74)\n",
    "\n",
    "        # Step 3: Freeze the base model layers if specified\n",
    "        for param in self.base_model.parameters():\n",
    "                param.requires_grad = False  # Freeze base model layers\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward pass through ResNet50 base model\n",
    "        x = self.base_model(x)  # Get features from ResNet50\n",
    "        x = self.fc1(x)          # Apply fully connected layer 1\n",
    "        x = self.batch_norm(x)   # Apply batch normalization after fc1\n",
    "        x = self.relu(x)         # ReLU activation\n",
    "        x = self.dropout(x)      # Apply dropout\n",
    "        x = self.fc2(x)          # Output layer\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = ResN50()\n",
    "# Assuming 'device' is the variable where the device is specified, e.g., 'cuda' or 'cpu'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the appropriate device (GPU if available, otherwise CPU)\n",
    "model = model.to(device)\n",
    "# Display model summary\n",
    "summary(model, input_size=(3, 224, 224))  # Input size (C, H, W) for EfficientNet-B3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7768ae81-7fce-4423-a840-f45ba83f4901",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c40b88e-6212-44cc-8b1f-078c52793cd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570327dc-5c75-4fb1-86e0-c8617547977e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0622d61d-53fa-4272-9b3c-30721b8f9c5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f704f91-5dfd-4217-abb0-618193aabfa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8682e0c2-0216-43dd-b457-a047f9c992bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818a76c2-0b32-4c68-a313-aac59a6d7b07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80af1157-a1ef-4fd1-8193-07297167849c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c463d8-1d6b-4ff0-addf-076f59e9252b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6920de2c-f362-4e0b-873d-4be617f86813",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927e47e7-57e9-4696-98e4-4d5c13d74ee3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d12626bd-e027-4838-98d1-0b1c84aa0e80",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 40, 112, 112]           1,080\n",
      "       BatchNorm2d-2         [-1, 40, 112, 112]              80\n",
      "              SiLU-3         [-1, 40, 112, 112]               0\n",
      "            Conv2d-4         [-1, 40, 112, 112]             360\n",
      "       BatchNorm2d-5         [-1, 40, 112, 112]              80\n",
      "              SiLU-6         [-1, 40, 112, 112]               0\n",
      " AdaptiveAvgPool2d-7             [-1, 40, 1, 1]               0\n",
      "            Conv2d-8             [-1, 10, 1, 1]             410\n",
      "              SiLU-9             [-1, 10, 1, 1]               0\n",
      "           Conv2d-10             [-1, 40, 1, 1]             440\n",
      "          Sigmoid-11             [-1, 40, 1, 1]               0\n",
      "SqueezeExcitation-12         [-1, 40, 112, 112]               0\n",
      "           Conv2d-13         [-1, 24, 112, 112]             960\n",
      "      BatchNorm2d-14         [-1, 24, 112, 112]              48\n",
      "           MBConv-15         [-1, 24, 112, 112]               0\n",
      "           Conv2d-16         [-1, 24, 112, 112]             216\n",
      "      BatchNorm2d-17         [-1, 24, 112, 112]              48\n",
      "             SiLU-18         [-1, 24, 112, 112]               0\n",
      "AdaptiveAvgPool2d-19             [-1, 24, 1, 1]               0\n",
      "           Conv2d-20              [-1, 6, 1, 1]             150\n",
      "             SiLU-21              [-1, 6, 1, 1]               0\n",
      "           Conv2d-22             [-1, 24, 1, 1]             168\n",
      "          Sigmoid-23             [-1, 24, 1, 1]               0\n",
      "SqueezeExcitation-24         [-1, 24, 112, 112]               0\n",
      "           Conv2d-25         [-1, 24, 112, 112]             576\n",
      "      BatchNorm2d-26         [-1, 24, 112, 112]              48\n",
      "  StochasticDepth-27         [-1, 24, 112, 112]               0\n",
      "           MBConv-28         [-1, 24, 112, 112]               0\n",
      "           Conv2d-29        [-1, 144, 112, 112]           3,456\n",
      "      BatchNorm2d-30        [-1, 144, 112, 112]             288\n",
      "             SiLU-31        [-1, 144, 112, 112]               0\n",
      "           Conv2d-32          [-1, 144, 56, 56]           1,296\n",
      "      BatchNorm2d-33          [-1, 144, 56, 56]             288\n",
      "             SiLU-34          [-1, 144, 56, 56]               0\n",
      "AdaptiveAvgPool2d-35            [-1, 144, 1, 1]               0\n",
      "           Conv2d-36              [-1, 6, 1, 1]             870\n",
      "             SiLU-37              [-1, 6, 1, 1]               0\n",
      "           Conv2d-38            [-1, 144, 1, 1]           1,008\n",
      "          Sigmoid-39            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-40          [-1, 144, 56, 56]               0\n",
      "           Conv2d-41           [-1, 32, 56, 56]           4,608\n",
      "      BatchNorm2d-42           [-1, 32, 56, 56]              64\n",
      "           MBConv-43           [-1, 32, 56, 56]               0\n",
      "           Conv2d-44          [-1, 192, 56, 56]           6,144\n",
      "      BatchNorm2d-45          [-1, 192, 56, 56]             384\n",
      "             SiLU-46          [-1, 192, 56, 56]               0\n",
      "           Conv2d-47          [-1, 192, 56, 56]           1,728\n",
      "      BatchNorm2d-48          [-1, 192, 56, 56]             384\n",
      "             SiLU-49          [-1, 192, 56, 56]               0\n",
      "AdaptiveAvgPool2d-50            [-1, 192, 1, 1]               0\n",
      "           Conv2d-51              [-1, 8, 1, 1]           1,544\n",
      "             SiLU-52              [-1, 8, 1, 1]               0\n",
      "           Conv2d-53            [-1, 192, 1, 1]           1,728\n",
      "          Sigmoid-54            [-1, 192, 1, 1]               0\n",
      "SqueezeExcitation-55          [-1, 192, 56, 56]               0\n",
      "           Conv2d-56           [-1, 32, 56, 56]           6,144\n",
      "      BatchNorm2d-57           [-1, 32, 56, 56]              64\n",
      "  StochasticDepth-58           [-1, 32, 56, 56]               0\n",
      "           MBConv-59           [-1, 32, 56, 56]               0\n",
      "           Conv2d-60          [-1, 192, 56, 56]           6,144\n",
      "      BatchNorm2d-61          [-1, 192, 56, 56]             384\n",
      "             SiLU-62          [-1, 192, 56, 56]               0\n",
      "           Conv2d-63          [-1, 192, 56, 56]           1,728\n",
      "      BatchNorm2d-64          [-1, 192, 56, 56]             384\n",
      "             SiLU-65          [-1, 192, 56, 56]               0\n",
      "AdaptiveAvgPool2d-66            [-1, 192, 1, 1]               0\n",
      "           Conv2d-67              [-1, 8, 1, 1]           1,544\n",
      "             SiLU-68              [-1, 8, 1, 1]               0\n",
      "           Conv2d-69            [-1, 192, 1, 1]           1,728\n",
      "          Sigmoid-70            [-1, 192, 1, 1]               0\n",
      "SqueezeExcitation-71          [-1, 192, 56, 56]               0\n",
      "           Conv2d-72           [-1, 32, 56, 56]           6,144\n",
      "      BatchNorm2d-73           [-1, 32, 56, 56]              64\n",
      "  StochasticDepth-74           [-1, 32, 56, 56]               0\n",
      "           MBConv-75           [-1, 32, 56, 56]               0\n",
      "           Conv2d-76          [-1, 192, 56, 56]           6,144\n",
      "      BatchNorm2d-77          [-1, 192, 56, 56]             384\n",
      "             SiLU-78          [-1, 192, 56, 56]               0\n",
      "           Conv2d-79          [-1, 192, 28, 28]           4,800\n",
      "      BatchNorm2d-80          [-1, 192, 28, 28]             384\n",
      "             SiLU-81          [-1, 192, 28, 28]               0\n",
      "AdaptiveAvgPool2d-82            [-1, 192, 1, 1]               0\n",
      "           Conv2d-83              [-1, 8, 1, 1]           1,544\n",
      "             SiLU-84              [-1, 8, 1, 1]               0\n",
      "           Conv2d-85            [-1, 192, 1, 1]           1,728\n",
      "          Sigmoid-86            [-1, 192, 1, 1]               0\n",
      "SqueezeExcitation-87          [-1, 192, 28, 28]               0\n",
      "           Conv2d-88           [-1, 48, 28, 28]           9,216\n",
      "      BatchNorm2d-89           [-1, 48, 28, 28]              96\n",
      "           MBConv-90           [-1, 48, 28, 28]               0\n",
      "           Conv2d-91          [-1, 288, 28, 28]          13,824\n",
      "      BatchNorm2d-92          [-1, 288, 28, 28]             576\n",
      "             SiLU-93          [-1, 288, 28, 28]               0\n",
      "           Conv2d-94          [-1, 288, 28, 28]           7,200\n",
      "      BatchNorm2d-95          [-1, 288, 28, 28]             576\n",
      "             SiLU-96          [-1, 288, 28, 28]               0\n",
      "AdaptiveAvgPool2d-97            [-1, 288, 1, 1]               0\n",
      "           Conv2d-98             [-1, 12, 1, 1]           3,468\n",
      "             SiLU-99             [-1, 12, 1, 1]               0\n",
      "          Conv2d-100            [-1, 288, 1, 1]           3,744\n",
      "         Sigmoid-101            [-1, 288, 1, 1]               0\n",
      "SqueezeExcitation-102          [-1, 288, 28, 28]               0\n",
      "          Conv2d-103           [-1, 48, 28, 28]          13,824\n",
      "     BatchNorm2d-104           [-1, 48, 28, 28]              96\n",
      " StochasticDepth-105           [-1, 48, 28, 28]               0\n",
      "          MBConv-106           [-1, 48, 28, 28]               0\n",
      "          Conv2d-107          [-1, 288, 28, 28]          13,824\n",
      "     BatchNorm2d-108          [-1, 288, 28, 28]             576\n",
      "            SiLU-109          [-1, 288, 28, 28]               0\n",
      "          Conv2d-110          [-1, 288, 28, 28]           7,200\n",
      "     BatchNorm2d-111          [-1, 288, 28, 28]             576\n",
      "            SiLU-112          [-1, 288, 28, 28]               0\n",
      "AdaptiveAvgPool2d-113            [-1, 288, 1, 1]               0\n",
      "          Conv2d-114             [-1, 12, 1, 1]           3,468\n",
      "            SiLU-115             [-1, 12, 1, 1]               0\n",
      "          Conv2d-116            [-1, 288, 1, 1]           3,744\n",
      "         Sigmoid-117            [-1, 288, 1, 1]               0\n",
      "SqueezeExcitation-118          [-1, 288, 28, 28]               0\n",
      "          Conv2d-119           [-1, 48, 28, 28]          13,824\n",
      "     BatchNorm2d-120           [-1, 48, 28, 28]              96\n",
      " StochasticDepth-121           [-1, 48, 28, 28]               0\n",
      "          MBConv-122           [-1, 48, 28, 28]               0\n",
      "          Conv2d-123          [-1, 288, 28, 28]          13,824\n",
      "     BatchNorm2d-124          [-1, 288, 28, 28]             576\n",
      "            SiLU-125          [-1, 288, 28, 28]               0\n",
      "          Conv2d-126          [-1, 288, 14, 14]           2,592\n",
      "     BatchNorm2d-127          [-1, 288, 14, 14]             576\n",
      "            SiLU-128          [-1, 288, 14, 14]               0\n",
      "AdaptiveAvgPool2d-129            [-1, 288, 1, 1]               0\n",
      "          Conv2d-130             [-1, 12, 1, 1]           3,468\n",
      "            SiLU-131             [-1, 12, 1, 1]               0\n",
      "          Conv2d-132            [-1, 288, 1, 1]           3,744\n",
      "         Sigmoid-133            [-1, 288, 1, 1]               0\n",
      "SqueezeExcitation-134          [-1, 288, 14, 14]               0\n",
      "          Conv2d-135           [-1, 96, 14, 14]          27,648\n",
      "     BatchNorm2d-136           [-1, 96, 14, 14]             192\n",
      "          MBConv-137           [-1, 96, 14, 14]               0\n",
      "          Conv2d-138          [-1, 576, 14, 14]          55,296\n",
      "     BatchNorm2d-139          [-1, 576, 14, 14]           1,152\n",
      "            SiLU-140          [-1, 576, 14, 14]               0\n",
      "          Conv2d-141          [-1, 576, 14, 14]           5,184\n",
      "     BatchNorm2d-142          [-1, 576, 14, 14]           1,152\n",
      "            SiLU-143          [-1, 576, 14, 14]               0\n",
      "AdaptiveAvgPool2d-144            [-1, 576, 1, 1]               0\n",
      "          Conv2d-145             [-1, 24, 1, 1]          13,848\n",
      "            SiLU-146             [-1, 24, 1, 1]               0\n",
      "          Conv2d-147            [-1, 576, 1, 1]          14,400\n",
      "         Sigmoid-148            [-1, 576, 1, 1]               0\n",
      "SqueezeExcitation-149          [-1, 576, 14, 14]               0\n",
      "          Conv2d-150           [-1, 96, 14, 14]          55,296\n",
      "     BatchNorm2d-151           [-1, 96, 14, 14]             192\n",
      " StochasticDepth-152           [-1, 96, 14, 14]               0\n",
      "          MBConv-153           [-1, 96, 14, 14]               0\n",
      "          Conv2d-154          [-1, 576, 14, 14]          55,296\n",
      "     BatchNorm2d-155          [-1, 576, 14, 14]           1,152\n",
      "            SiLU-156          [-1, 576, 14, 14]               0\n",
      "          Conv2d-157          [-1, 576, 14, 14]           5,184\n",
      "     BatchNorm2d-158          [-1, 576, 14, 14]           1,152\n",
      "            SiLU-159          [-1, 576, 14, 14]               0\n",
      "AdaptiveAvgPool2d-160            [-1, 576, 1, 1]               0\n",
      "          Conv2d-161             [-1, 24, 1, 1]          13,848\n",
      "            SiLU-162             [-1, 24, 1, 1]               0\n",
      "          Conv2d-163            [-1, 576, 1, 1]          14,400\n",
      "         Sigmoid-164            [-1, 576, 1, 1]               0\n",
      "SqueezeExcitation-165          [-1, 576, 14, 14]               0\n",
      "          Conv2d-166           [-1, 96, 14, 14]          55,296\n",
      "     BatchNorm2d-167           [-1, 96, 14, 14]             192\n",
      " StochasticDepth-168           [-1, 96, 14, 14]               0\n",
      "          MBConv-169           [-1, 96, 14, 14]               0\n",
      "          Conv2d-170          [-1, 576, 14, 14]          55,296\n",
      "     BatchNorm2d-171          [-1, 576, 14, 14]           1,152\n",
      "            SiLU-172          [-1, 576, 14, 14]               0\n",
      "          Conv2d-173          [-1, 576, 14, 14]           5,184\n",
      "     BatchNorm2d-174          [-1, 576, 14, 14]           1,152\n",
      "            SiLU-175          [-1, 576, 14, 14]               0\n",
      "AdaptiveAvgPool2d-176            [-1, 576, 1, 1]               0\n",
      "          Conv2d-177             [-1, 24, 1, 1]          13,848\n",
      "            SiLU-178             [-1, 24, 1, 1]               0\n",
      "          Conv2d-179            [-1, 576, 1, 1]          14,400\n",
      "         Sigmoid-180            [-1, 576, 1, 1]               0\n",
      "SqueezeExcitation-181          [-1, 576, 14, 14]               0\n",
      "          Conv2d-182           [-1, 96, 14, 14]          55,296\n",
      "     BatchNorm2d-183           [-1, 96, 14, 14]             192\n",
      " StochasticDepth-184           [-1, 96, 14, 14]               0\n",
      "          MBConv-185           [-1, 96, 14, 14]               0\n",
      "          Conv2d-186          [-1, 576, 14, 14]          55,296\n",
      "     BatchNorm2d-187          [-1, 576, 14, 14]           1,152\n",
      "            SiLU-188          [-1, 576, 14, 14]               0\n",
      "          Conv2d-189          [-1, 576, 14, 14]           5,184\n",
      "     BatchNorm2d-190          [-1, 576, 14, 14]           1,152\n",
      "            SiLU-191          [-1, 576, 14, 14]               0\n",
      "AdaptiveAvgPool2d-192            [-1, 576, 1, 1]               0\n",
      "          Conv2d-193             [-1, 24, 1, 1]          13,848\n",
      "            SiLU-194             [-1, 24, 1, 1]               0\n",
      "          Conv2d-195            [-1, 576, 1, 1]          14,400\n",
      "         Sigmoid-196            [-1, 576, 1, 1]               0\n",
      "SqueezeExcitation-197          [-1, 576, 14, 14]               0\n",
      "          Conv2d-198           [-1, 96, 14, 14]          55,296\n",
      "     BatchNorm2d-199           [-1, 96, 14, 14]             192\n",
      " StochasticDepth-200           [-1, 96, 14, 14]               0\n",
      "          MBConv-201           [-1, 96, 14, 14]               0\n",
      "          Conv2d-202          [-1, 576, 14, 14]          55,296\n",
      "     BatchNorm2d-203          [-1, 576, 14, 14]           1,152\n",
      "            SiLU-204          [-1, 576, 14, 14]               0\n",
      "          Conv2d-205          [-1, 576, 14, 14]          14,400\n",
      "     BatchNorm2d-206          [-1, 576, 14, 14]           1,152\n",
      "            SiLU-207          [-1, 576, 14, 14]               0\n",
      "AdaptiveAvgPool2d-208            [-1, 576, 1, 1]               0\n",
      "          Conv2d-209             [-1, 24, 1, 1]          13,848\n",
      "            SiLU-210             [-1, 24, 1, 1]               0\n",
      "          Conv2d-211            [-1, 576, 1, 1]          14,400\n",
      "         Sigmoid-212            [-1, 576, 1, 1]               0\n",
      "SqueezeExcitation-213          [-1, 576, 14, 14]               0\n",
      "          Conv2d-214          [-1, 136, 14, 14]          78,336\n",
      "     BatchNorm2d-215          [-1, 136, 14, 14]             272\n",
      "          MBConv-216          [-1, 136, 14, 14]               0\n",
      "          Conv2d-217          [-1, 816, 14, 14]         110,976\n",
      "     BatchNorm2d-218          [-1, 816, 14, 14]           1,632\n",
      "            SiLU-219          [-1, 816, 14, 14]               0\n",
      "          Conv2d-220          [-1, 816, 14, 14]          20,400\n",
      "     BatchNorm2d-221          [-1, 816, 14, 14]           1,632\n",
      "            SiLU-222          [-1, 816, 14, 14]               0\n",
      "AdaptiveAvgPool2d-223            [-1, 816, 1, 1]               0\n",
      "          Conv2d-224             [-1, 34, 1, 1]          27,778\n",
      "            SiLU-225             [-1, 34, 1, 1]               0\n",
      "          Conv2d-226            [-1, 816, 1, 1]          28,560\n",
      "         Sigmoid-227            [-1, 816, 1, 1]               0\n",
      "SqueezeExcitation-228          [-1, 816, 14, 14]               0\n",
      "          Conv2d-229          [-1, 136, 14, 14]         110,976\n",
      "     BatchNorm2d-230          [-1, 136, 14, 14]             272\n",
      " StochasticDepth-231          [-1, 136, 14, 14]               0\n",
      "          MBConv-232          [-1, 136, 14, 14]               0\n",
      "          Conv2d-233          [-1, 816, 14, 14]         110,976\n",
      "     BatchNorm2d-234          [-1, 816, 14, 14]           1,632\n",
      "            SiLU-235          [-1, 816, 14, 14]               0\n",
      "          Conv2d-236          [-1, 816, 14, 14]          20,400\n",
      "     BatchNorm2d-237          [-1, 816, 14, 14]           1,632\n",
      "            SiLU-238          [-1, 816, 14, 14]               0\n",
      "AdaptiveAvgPool2d-239            [-1, 816, 1, 1]               0\n",
      "          Conv2d-240             [-1, 34, 1, 1]          27,778\n",
      "            SiLU-241             [-1, 34, 1, 1]               0\n",
      "          Conv2d-242            [-1, 816, 1, 1]          28,560\n",
      "         Sigmoid-243            [-1, 816, 1, 1]               0\n",
      "SqueezeExcitation-244          [-1, 816, 14, 14]               0\n",
      "          Conv2d-245          [-1, 136, 14, 14]         110,976\n",
      "     BatchNorm2d-246          [-1, 136, 14, 14]             272\n",
      " StochasticDepth-247          [-1, 136, 14, 14]               0\n",
      "          MBConv-248          [-1, 136, 14, 14]               0\n",
      "          Conv2d-249          [-1, 816, 14, 14]         110,976\n",
      "     BatchNorm2d-250          [-1, 816, 14, 14]           1,632\n",
      "            SiLU-251          [-1, 816, 14, 14]               0\n",
      "          Conv2d-252          [-1, 816, 14, 14]          20,400\n",
      "     BatchNorm2d-253          [-1, 816, 14, 14]           1,632\n",
      "            SiLU-254          [-1, 816, 14, 14]               0\n",
      "AdaptiveAvgPool2d-255            [-1, 816, 1, 1]               0\n",
      "          Conv2d-256             [-1, 34, 1, 1]          27,778\n",
      "            SiLU-257             [-1, 34, 1, 1]               0\n",
      "          Conv2d-258            [-1, 816, 1, 1]          28,560\n",
      "         Sigmoid-259            [-1, 816, 1, 1]               0\n",
      "SqueezeExcitation-260          [-1, 816, 14, 14]               0\n",
      "          Conv2d-261          [-1, 136, 14, 14]         110,976\n",
      "     BatchNorm2d-262          [-1, 136, 14, 14]             272\n",
      " StochasticDepth-263          [-1, 136, 14, 14]               0\n",
      "          MBConv-264          [-1, 136, 14, 14]               0\n",
      "          Conv2d-265          [-1, 816, 14, 14]         110,976\n",
      "     BatchNorm2d-266          [-1, 816, 14, 14]           1,632\n",
      "            SiLU-267          [-1, 816, 14, 14]               0\n",
      "          Conv2d-268          [-1, 816, 14, 14]          20,400\n",
      "     BatchNorm2d-269          [-1, 816, 14, 14]           1,632\n",
      "            SiLU-270          [-1, 816, 14, 14]               0\n",
      "AdaptiveAvgPool2d-271            [-1, 816, 1, 1]               0\n",
      "          Conv2d-272             [-1, 34, 1, 1]          27,778\n",
      "            SiLU-273             [-1, 34, 1, 1]               0\n",
      "          Conv2d-274            [-1, 816, 1, 1]          28,560\n",
      "         Sigmoid-275            [-1, 816, 1, 1]               0\n",
      "SqueezeExcitation-276          [-1, 816, 14, 14]               0\n",
      "          Conv2d-277          [-1, 136, 14, 14]         110,976\n",
      "     BatchNorm2d-278          [-1, 136, 14, 14]             272\n",
      " StochasticDepth-279          [-1, 136, 14, 14]               0\n",
      "          MBConv-280          [-1, 136, 14, 14]               0\n",
      "          Conv2d-281          [-1, 816, 14, 14]         110,976\n",
      "     BatchNorm2d-282          [-1, 816, 14, 14]           1,632\n",
      "            SiLU-283          [-1, 816, 14, 14]               0\n",
      "          Conv2d-284            [-1, 816, 7, 7]          20,400\n",
      "     BatchNorm2d-285            [-1, 816, 7, 7]           1,632\n",
      "            SiLU-286            [-1, 816, 7, 7]               0\n",
      "AdaptiveAvgPool2d-287            [-1, 816, 1, 1]               0\n",
      "          Conv2d-288             [-1, 34, 1, 1]          27,778\n",
      "            SiLU-289             [-1, 34, 1, 1]               0\n",
      "          Conv2d-290            [-1, 816, 1, 1]          28,560\n",
      "         Sigmoid-291            [-1, 816, 1, 1]               0\n",
      "SqueezeExcitation-292            [-1, 816, 7, 7]               0\n",
      "          Conv2d-293            [-1, 232, 7, 7]         189,312\n",
      "     BatchNorm2d-294            [-1, 232, 7, 7]             464\n",
      "          MBConv-295            [-1, 232, 7, 7]               0\n",
      "          Conv2d-296           [-1, 1392, 7, 7]         322,944\n",
      "     BatchNorm2d-297           [-1, 1392, 7, 7]           2,784\n",
      "            SiLU-298           [-1, 1392, 7, 7]               0\n",
      "          Conv2d-299           [-1, 1392, 7, 7]          34,800\n",
      "     BatchNorm2d-300           [-1, 1392, 7, 7]           2,784\n",
      "            SiLU-301           [-1, 1392, 7, 7]               0\n",
      "AdaptiveAvgPool2d-302           [-1, 1392, 1, 1]               0\n",
      "          Conv2d-303             [-1, 58, 1, 1]          80,794\n",
      "            SiLU-304             [-1, 58, 1, 1]               0\n",
      "          Conv2d-305           [-1, 1392, 1, 1]          82,128\n",
      "         Sigmoid-306           [-1, 1392, 1, 1]               0\n",
      "SqueezeExcitation-307           [-1, 1392, 7, 7]               0\n",
      "          Conv2d-308            [-1, 232, 7, 7]         322,944\n",
      "     BatchNorm2d-309            [-1, 232, 7, 7]             464\n",
      " StochasticDepth-310            [-1, 232, 7, 7]               0\n",
      "          MBConv-311            [-1, 232, 7, 7]               0\n",
      "          Conv2d-312           [-1, 1392, 7, 7]         322,944\n",
      "     BatchNorm2d-313           [-1, 1392, 7, 7]           2,784\n",
      "            SiLU-314           [-1, 1392, 7, 7]               0\n",
      "          Conv2d-315           [-1, 1392, 7, 7]          34,800\n",
      "     BatchNorm2d-316           [-1, 1392, 7, 7]           2,784\n",
      "            SiLU-317           [-1, 1392, 7, 7]               0\n",
      "AdaptiveAvgPool2d-318           [-1, 1392, 1, 1]               0\n",
      "          Conv2d-319             [-1, 58, 1, 1]          80,794\n",
      "            SiLU-320             [-1, 58, 1, 1]               0\n",
      "          Conv2d-321           [-1, 1392, 1, 1]          82,128\n",
      "         Sigmoid-322           [-1, 1392, 1, 1]               0\n",
      "SqueezeExcitation-323           [-1, 1392, 7, 7]               0\n",
      "          Conv2d-324            [-1, 232, 7, 7]         322,944\n",
      "     BatchNorm2d-325            [-1, 232, 7, 7]             464\n",
      " StochasticDepth-326            [-1, 232, 7, 7]               0\n",
      "          MBConv-327            [-1, 232, 7, 7]               0\n",
      "          Conv2d-328           [-1, 1392, 7, 7]         322,944\n",
      "     BatchNorm2d-329           [-1, 1392, 7, 7]           2,784\n",
      "            SiLU-330           [-1, 1392, 7, 7]               0\n",
      "          Conv2d-331           [-1, 1392, 7, 7]          34,800\n",
      "     BatchNorm2d-332           [-1, 1392, 7, 7]           2,784\n",
      "            SiLU-333           [-1, 1392, 7, 7]               0\n",
      "AdaptiveAvgPool2d-334           [-1, 1392, 1, 1]               0\n",
      "          Conv2d-335             [-1, 58, 1, 1]          80,794\n",
      "            SiLU-336             [-1, 58, 1, 1]               0\n",
      "          Conv2d-337           [-1, 1392, 1, 1]          82,128\n",
      "         Sigmoid-338           [-1, 1392, 1, 1]               0\n",
      "SqueezeExcitation-339           [-1, 1392, 7, 7]               0\n",
      "          Conv2d-340            [-1, 232, 7, 7]         322,944\n",
      "     BatchNorm2d-341            [-1, 232, 7, 7]             464\n",
      " StochasticDepth-342            [-1, 232, 7, 7]               0\n",
      "          MBConv-343            [-1, 232, 7, 7]               0\n",
      "          Conv2d-344           [-1, 1392, 7, 7]         322,944\n",
      "     BatchNorm2d-345           [-1, 1392, 7, 7]           2,784\n",
      "            SiLU-346           [-1, 1392, 7, 7]               0\n",
      "          Conv2d-347           [-1, 1392, 7, 7]          34,800\n",
      "     BatchNorm2d-348           [-1, 1392, 7, 7]           2,784\n",
      "            SiLU-349           [-1, 1392, 7, 7]               0\n",
      "AdaptiveAvgPool2d-350           [-1, 1392, 1, 1]               0\n",
      "          Conv2d-351             [-1, 58, 1, 1]          80,794\n",
      "            SiLU-352             [-1, 58, 1, 1]               0\n",
      "          Conv2d-353           [-1, 1392, 1, 1]          82,128\n",
      "         Sigmoid-354           [-1, 1392, 1, 1]               0\n",
      "SqueezeExcitation-355           [-1, 1392, 7, 7]               0\n",
      "          Conv2d-356            [-1, 232, 7, 7]         322,944\n",
      "     BatchNorm2d-357            [-1, 232, 7, 7]             464\n",
      " StochasticDepth-358            [-1, 232, 7, 7]               0\n",
      "          MBConv-359            [-1, 232, 7, 7]               0\n",
      "          Conv2d-360           [-1, 1392, 7, 7]         322,944\n",
      "     BatchNorm2d-361           [-1, 1392, 7, 7]           2,784\n",
      "            SiLU-362           [-1, 1392, 7, 7]               0\n",
      "          Conv2d-363           [-1, 1392, 7, 7]          34,800\n",
      "     BatchNorm2d-364           [-1, 1392, 7, 7]           2,784\n",
      "            SiLU-365           [-1, 1392, 7, 7]               0\n",
      "AdaptiveAvgPool2d-366           [-1, 1392, 1, 1]               0\n",
      "          Conv2d-367             [-1, 58, 1, 1]          80,794\n",
      "            SiLU-368             [-1, 58, 1, 1]               0\n",
      "          Conv2d-369           [-1, 1392, 1, 1]          82,128\n",
      "         Sigmoid-370           [-1, 1392, 1, 1]               0\n",
      "SqueezeExcitation-371           [-1, 1392, 7, 7]               0\n",
      "          Conv2d-372            [-1, 232, 7, 7]         322,944\n",
      "     BatchNorm2d-373            [-1, 232, 7, 7]             464\n",
      " StochasticDepth-374            [-1, 232, 7, 7]               0\n",
      "          MBConv-375            [-1, 232, 7, 7]               0\n",
      "          Conv2d-376           [-1, 1392, 7, 7]         322,944\n",
      "     BatchNorm2d-377           [-1, 1392, 7, 7]           2,784\n",
      "            SiLU-378           [-1, 1392, 7, 7]               0\n",
      "          Conv2d-379           [-1, 1392, 7, 7]          12,528\n",
      "     BatchNorm2d-380           [-1, 1392, 7, 7]           2,784\n",
      "            SiLU-381           [-1, 1392, 7, 7]               0\n",
      "AdaptiveAvgPool2d-382           [-1, 1392, 1, 1]               0\n",
      "          Conv2d-383             [-1, 58, 1, 1]          80,794\n",
      "            SiLU-384             [-1, 58, 1, 1]               0\n",
      "          Conv2d-385           [-1, 1392, 1, 1]          82,128\n",
      "         Sigmoid-386           [-1, 1392, 1, 1]               0\n",
      "SqueezeExcitation-387           [-1, 1392, 7, 7]               0\n",
      "          Conv2d-388            [-1, 384, 7, 7]         534,528\n",
      "     BatchNorm2d-389            [-1, 384, 7, 7]             768\n",
      "          MBConv-390            [-1, 384, 7, 7]               0\n",
      "          Conv2d-391           [-1, 2304, 7, 7]         884,736\n",
      "     BatchNorm2d-392           [-1, 2304, 7, 7]           4,608\n",
      "            SiLU-393           [-1, 2304, 7, 7]               0\n",
      "          Conv2d-394           [-1, 2304, 7, 7]          20,736\n",
      "     BatchNorm2d-395           [-1, 2304, 7, 7]           4,608\n",
      "            SiLU-396           [-1, 2304, 7, 7]               0\n",
      "AdaptiveAvgPool2d-397           [-1, 2304, 1, 1]               0\n",
      "          Conv2d-398             [-1, 96, 1, 1]         221,280\n",
      "            SiLU-399             [-1, 96, 1, 1]               0\n",
      "          Conv2d-400           [-1, 2304, 1, 1]         223,488\n",
      "         Sigmoid-401           [-1, 2304, 1, 1]               0\n",
      "SqueezeExcitation-402           [-1, 2304, 7, 7]               0\n",
      "          Conv2d-403            [-1, 384, 7, 7]         884,736\n",
      "     BatchNorm2d-404            [-1, 384, 7, 7]             768\n",
      " StochasticDepth-405            [-1, 384, 7, 7]               0\n",
      "          MBConv-406            [-1, 384, 7, 7]               0\n",
      "          Conv2d-407           [-1, 1536, 7, 7]         589,824\n",
      "     BatchNorm2d-408           [-1, 1536, 7, 7]           3,072\n",
      "            SiLU-409           [-1, 1536, 7, 7]               0\n",
      "AdaptiveAvgPool2d-410           [-1, 1536, 1, 1]               0\n",
      "        Identity-411                 [-1, 1536]               0\n",
      "    EfficientNet-412                 [-1, 1536]               0\n",
      "     BatchNorm1d-413                 [-1, 1536]           3,072\n",
      "          Linear-414                  [-1, 256]         393,472\n",
      "            ReLU-415                  [-1, 256]               0\n",
      "         Dropout-416                  [-1, 256]               0\n",
      "          Linear-417                   [-1, 74]          19,018\n",
      "================================================================\n",
      "Total params: 11,111,794\n",
      "Trainable params: 415,562\n",
      "Non-trainable params: 10,696,232\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 341.36\n",
      "Params size (MB): 42.39\n",
      "Estimated Total Size (MB): 384.32\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class EfficientNetB3Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EfficientNetB3Model, self).__init__()\n",
    "        \n",
    "        # Step 1: Load EfficientNet-B3 base model (without pre-trained weights)\n",
    "        self.base_model = models.efficientnet_b3(weights=None)  # No pretrained weights at first\n",
    "        self.base_model.classifier = nn.Identity()  # Remove final classification layer\n",
    "        \n",
    "        # Step 2: Load the pre-trained weights into the base model\n",
    "        state_dict = torch.load(\"preTrained/efficientnet_b3_weights.pth\", weights_only=True)\n",
    "        # Remove classifier weights from state_dict to avoid the mismatch\n",
    "        state_dict = {k: v for k, v in state_dict.items() if 'classifier' not in k}\n",
    "        \n",
    "        # Load the filtered state_dict into the model\n",
    "        self.base_model.load_state_dict(state_dict)\n",
    "        # print(\"EfficientNet-B3 weights loaded successfully.\")\n",
    "        \n",
    "        # Custom layers added after the base model\n",
    "        self.batch_norm = nn.BatchNorm1d(1536)  # Output size of EfficientNet-B3 before classifier is 1536\n",
    "        self.fc1 = nn.Linear(1536, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(256, 74)\n",
    "        \n",
    "        # Step 3: Freeze the base model layers if specified\n",
    "        for param in self.base_model.parameters():\n",
    "                param.requires_grad = False  # Freeze base model layers\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward pass through EfficientNet-B3 base model\n",
    "        x = self.base_model(x)  # Get features from EfficientNet-B3\n",
    "        x = self.batch_norm(x)   # Apply batch normalization\n",
    "        x = self.fc1(x)          # Apply fully connected layer 1\n",
    "        x = self.relu(x)         # ReLU activation\n",
    "        x = self.dropout(x)      # Apply dropout\n",
    "        x = self.fc2(x)          # Output layer\n",
    "        return x\n",
    "\n",
    "# Initialize the model (freeze base model layers by default)\n",
    "model = EfficientNetB3Model()\n",
    "# Assuming 'device' is the variable where the device is specified, e.g., 'cuda' or 'cpu'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the appropriate device (GPU if available, otherwise CPU)\n",
    "model = model.to(device)\n",
    "# Display model summary\n",
    "summary(model, input_size=(3, 224, 224))  # Input size (C, H, W) for EfficientNet-B3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7fb2381-03c6-402c-83dd-c829fc8c8b48",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 40, 112, 112]           1,080\n",
      "       BatchNorm2d-2         [-1, 40, 112, 112]              80\n",
      "              SiLU-3         [-1, 40, 112, 112]               0\n",
      "            Conv2d-4         [-1, 40, 112, 112]             360\n",
      "       BatchNorm2d-5         [-1, 40, 112, 112]              80\n",
      "              SiLU-6         [-1, 40, 112, 112]               0\n",
      " AdaptiveAvgPool2d-7             [-1, 40, 1, 1]               0\n",
      "            Conv2d-8             [-1, 10, 1, 1]             410\n",
      "              SiLU-9             [-1, 10, 1, 1]               0\n",
      "           Conv2d-10             [-1, 40, 1, 1]             440\n",
      "          Sigmoid-11             [-1, 40, 1, 1]               0\n",
      "SqueezeExcitation-12         [-1, 40, 112, 112]               0\n",
      "           Conv2d-13         [-1, 24, 112, 112]             960\n",
      "      BatchNorm2d-14         [-1, 24, 112, 112]              48\n",
      "           MBConv-15         [-1, 24, 112, 112]               0\n",
      "           Conv2d-16         [-1, 24, 112, 112]             216\n",
      "      BatchNorm2d-17         [-1, 24, 112, 112]              48\n",
      "             SiLU-18         [-1, 24, 112, 112]               0\n",
      "AdaptiveAvgPool2d-19             [-1, 24, 1, 1]               0\n",
      "           Conv2d-20              [-1, 6, 1, 1]             150\n",
      "             SiLU-21              [-1, 6, 1, 1]               0\n",
      "           Conv2d-22             [-1, 24, 1, 1]             168\n",
      "          Sigmoid-23             [-1, 24, 1, 1]               0\n",
      "SqueezeExcitation-24         [-1, 24, 112, 112]               0\n",
      "           Conv2d-25         [-1, 24, 112, 112]             576\n",
      "      BatchNorm2d-26         [-1, 24, 112, 112]              48\n",
      "  StochasticDepth-27         [-1, 24, 112, 112]               0\n",
      "           MBConv-28         [-1, 24, 112, 112]               0\n",
      "           Conv2d-29        [-1, 144, 112, 112]           3,456\n",
      "      BatchNorm2d-30        [-1, 144, 112, 112]             288\n",
      "             SiLU-31        [-1, 144, 112, 112]               0\n",
      "           Conv2d-32          [-1, 144, 56, 56]           1,296\n",
      "      BatchNorm2d-33          [-1, 144, 56, 56]             288\n",
      "             SiLU-34          [-1, 144, 56, 56]               0\n",
      "AdaptiveAvgPool2d-35            [-1, 144, 1, 1]               0\n",
      "           Conv2d-36              [-1, 6, 1, 1]             870\n",
      "             SiLU-37              [-1, 6, 1, 1]               0\n",
      "           Conv2d-38            [-1, 144, 1, 1]           1,008\n",
      "          Sigmoid-39            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-40          [-1, 144, 56, 56]               0\n",
      "           Conv2d-41           [-1, 32, 56, 56]           4,608\n",
      "      BatchNorm2d-42           [-1, 32, 56, 56]              64\n",
      "           MBConv-43           [-1, 32, 56, 56]               0\n",
      "           Conv2d-44          [-1, 192, 56, 56]           6,144\n",
      "      BatchNorm2d-45          [-1, 192, 56, 56]             384\n",
      "             SiLU-46          [-1, 192, 56, 56]               0\n",
      "           Conv2d-47          [-1, 192, 56, 56]           1,728\n",
      "      BatchNorm2d-48          [-1, 192, 56, 56]             384\n",
      "             SiLU-49          [-1, 192, 56, 56]               0\n",
      "AdaptiveAvgPool2d-50            [-1, 192, 1, 1]               0\n",
      "           Conv2d-51              [-1, 8, 1, 1]           1,544\n",
      "             SiLU-52              [-1, 8, 1, 1]               0\n",
      "           Conv2d-53            [-1, 192, 1, 1]           1,728\n",
      "          Sigmoid-54            [-1, 192, 1, 1]               0\n",
      "SqueezeExcitation-55          [-1, 192, 56, 56]               0\n",
      "           Conv2d-56           [-1, 32, 56, 56]           6,144\n",
      "      BatchNorm2d-57           [-1, 32, 56, 56]              64\n",
      "  StochasticDepth-58           [-1, 32, 56, 56]               0\n",
      "           MBConv-59           [-1, 32, 56, 56]               0\n",
      "           Conv2d-60          [-1, 192, 56, 56]           6,144\n",
      "      BatchNorm2d-61          [-1, 192, 56, 56]             384\n",
      "             SiLU-62          [-1, 192, 56, 56]               0\n",
      "           Conv2d-63          [-1, 192, 56, 56]           1,728\n",
      "      BatchNorm2d-64          [-1, 192, 56, 56]             384\n",
      "             SiLU-65          [-1, 192, 56, 56]               0\n",
      "AdaptiveAvgPool2d-66            [-1, 192, 1, 1]               0\n",
      "           Conv2d-67              [-1, 8, 1, 1]           1,544\n",
      "             SiLU-68              [-1, 8, 1, 1]               0\n",
      "           Conv2d-69            [-1, 192, 1, 1]           1,728\n",
      "          Sigmoid-70            [-1, 192, 1, 1]               0\n",
      "SqueezeExcitation-71          [-1, 192, 56, 56]               0\n",
      "           Conv2d-72           [-1, 32, 56, 56]           6,144\n",
      "      BatchNorm2d-73           [-1, 32, 56, 56]              64\n",
      "  StochasticDepth-74           [-1, 32, 56, 56]               0\n",
      "           MBConv-75           [-1, 32, 56, 56]               0\n",
      "           Conv2d-76          [-1, 192, 56, 56]           6,144\n",
      "      BatchNorm2d-77          [-1, 192, 56, 56]             384\n",
      "             SiLU-78          [-1, 192, 56, 56]               0\n",
      "           Conv2d-79          [-1, 192, 28, 28]           4,800\n",
      "      BatchNorm2d-80          [-1, 192, 28, 28]             384\n",
      "             SiLU-81          [-1, 192, 28, 28]               0\n",
      "AdaptiveAvgPool2d-82            [-1, 192, 1, 1]               0\n",
      "           Conv2d-83              [-1, 8, 1, 1]           1,544\n",
      "             SiLU-84              [-1, 8, 1, 1]               0\n",
      "           Conv2d-85            [-1, 192, 1, 1]           1,728\n",
      "          Sigmoid-86            [-1, 192, 1, 1]               0\n",
      "SqueezeExcitation-87          [-1, 192, 28, 28]               0\n",
      "           Conv2d-88           [-1, 48, 28, 28]           9,216\n",
      "      BatchNorm2d-89           [-1, 48, 28, 28]              96\n",
      "           MBConv-90           [-1, 48, 28, 28]               0\n",
      "           Conv2d-91          [-1, 288, 28, 28]          13,824\n",
      "      BatchNorm2d-92          [-1, 288, 28, 28]             576\n",
      "             SiLU-93          [-1, 288, 28, 28]               0\n",
      "           Conv2d-94          [-1, 288, 28, 28]           7,200\n",
      "      BatchNorm2d-95          [-1, 288, 28, 28]             576\n",
      "             SiLU-96          [-1, 288, 28, 28]               0\n",
      "AdaptiveAvgPool2d-97            [-1, 288, 1, 1]               0\n",
      "           Conv2d-98             [-1, 12, 1, 1]           3,468\n",
      "             SiLU-99             [-1, 12, 1, 1]               0\n",
      "          Conv2d-100            [-1, 288, 1, 1]           3,744\n",
      "         Sigmoid-101            [-1, 288, 1, 1]               0\n",
      "SqueezeExcitation-102          [-1, 288, 28, 28]               0\n",
      "          Conv2d-103           [-1, 48, 28, 28]          13,824\n",
      "     BatchNorm2d-104           [-1, 48, 28, 28]              96\n",
      " StochasticDepth-105           [-1, 48, 28, 28]               0\n",
      "          MBConv-106           [-1, 48, 28, 28]               0\n",
      "          Conv2d-107          [-1, 288, 28, 28]          13,824\n",
      "     BatchNorm2d-108          [-1, 288, 28, 28]             576\n",
      "            SiLU-109          [-1, 288, 28, 28]               0\n",
      "          Conv2d-110          [-1, 288, 28, 28]           7,200\n",
      "     BatchNorm2d-111          [-1, 288, 28, 28]             576\n",
      "            SiLU-112          [-1, 288, 28, 28]               0\n",
      "AdaptiveAvgPool2d-113            [-1, 288, 1, 1]               0\n",
      "          Conv2d-114             [-1, 12, 1, 1]           3,468\n",
      "            SiLU-115             [-1, 12, 1, 1]               0\n",
      "          Conv2d-116            [-1, 288, 1, 1]           3,744\n",
      "         Sigmoid-117            [-1, 288, 1, 1]               0\n",
      "SqueezeExcitation-118          [-1, 288, 28, 28]               0\n",
      "          Conv2d-119           [-1, 48, 28, 28]          13,824\n",
      "     BatchNorm2d-120           [-1, 48, 28, 28]              96\n",
      " StochasticDepth-121           [-1, 48, 28, 28]               0\n",
      "          MBConv-122           [-1, 48, 28, 28]               0\n",
      "          Conv2d-123          [-1, 288, 28, 28]          13,824\n",
      "     BatchNorm2d-124          [-1, 288, 28, 28]             576\n",
      "            SiLU-125          [-1, 288, 28, 28]               0\n",
      "          Conv2d-126          [-1, 288, 14, 14]           2,592\n",
      "     BatchNorm2d-127          [-1, 288, 14, 14]             576\n",
      "            SiLU-128          [-1, 288, 14, 14]               0\n",
      "AdaptiveAvgPool2d-129            [-1, 288, 1, 1]               0\n",
      "          Conv2d-130             [-1, 12, 1, 1]           3,468\n",
      "            SiLU-131             [-1, 12, 1, 1]               0\n",
      "          Conv2d-132            [-1, 288, 1, 1]           3,744\n",
      "         Sigmoid-133            [-1, 288, 1, 1]               0\n",
      "SqueezeExcitation-134          [-1, 288, 14, 14]               0\n",
      "          Conv2d-135           [-1, 96, 14, 14]          27,648\n",
      "     BatchNorm2d-136           [-1, 96, 14, 14]             192\n",
      "          MBConv-137           [-1, 96, 14, 14]               0\n",
      "          Conv2d-138          [-1, 576, 14, 14]          55,296\n",
      "     BatchNorm2d-139          [-1, 576, 14, 14]           1,152\n",
      "            SiLU-140          [-1, 576, 14, 14]               0\n",
      "          Conv2d-141          [-1, 576, 14, 14]           5,184\n",
      "     BatchNorm2d-142          [-1, 576, 14, 14]           1,152\n",
      "            SiLU-143          [-1, 576, 14, 14]               0\n",
      "AdaptiveAvgPool2d-144            [-1, 576, 1, 1]               0\n",
      "          Conv2d-145             [-1, 24, 1, 1]          13,848\n",
      "            SiLU-146             [-1, 24, 1, 1]               0\n",
      "          Conv2d-147            [-1, 576, 1, 1]          14,400\n",
      "         Sigmoid-148            [-1, 576, 1, 1]               0\n",
      "SqueezeExcitation-149          [-1, 576, 14, 14]               0\n",
      "          Conv2d-150           [-1, 96, 14, 14]          55,296\n",
      "     BatchNorm2d-151           [-1, 96, 14, 14]             192\n",
      " StochasticDepth-152           [-1, 96, 14, 14]               0\n",
      "          MBConv-153           [-1, 96, 14, 14]               0\n",
      "          Conv2d-154          [-1, 576, 14, 14]          55,296\n",
      "     BatchNorm2d-155          [-1, 576, 14, 14]           1,152\n",
      "            SiLU-156          [-1, 576, 14, 14]               0\n",
      "          Conv2d-157          [-1, 576, 14, 14]           5,184\n",
      "     BatchNorm2d-158          [-1, 576, 14, 14]           1,152\n",
      "            SiLU-159          [-1, 576, 14, 14]               0\n",
      "AdaptiveAvgPool2d-160            [-1, 576, 1, 1]               0\n",
      "          Conv2d-161             [-1, 24, 1, 1]          13,848\n",
      "            SiLU-162             [-1, 24, 1, 1]               0\n",
      "          Conv2d-163            [-1, 576, 1, 1]          14,400\n",
      "         Sigmoid-164            [-1, 576, 1, 1]               0\n",
      "SqueezeExcitation-165          [-1, 576, 14, 14]               0\n",
      "          Conv2d-166           [-1, 96, 14, 14]          55,296\n",
      "     BatchNorm2d-167           [-1, 96, 14, 14]             192\n",
      " StochasticDepth-168           [-1, 96, 14, 14]               0\n",
      "          MBConv-169           [-1, 96, 14, 14]               0\n",
      "          Conv2d-170          [-1, 576, 14, 14]          55,296\n",
      "     BatchNorm2d-171          [-1, 576, 14, 14]           1,152\n",
      "            SiLU-172          [-1, 576, 14, 14]               0\n",
      "          Conv2d-173          [-1, 576, 14, 14]           5,184\n",
      "     BatchNorm2d-174          [-1, 576, 14, 14]           1,152\n",
      "            SiLU-175          [-1, 576, 14, 14]               0\n",
      "AdaptiveAvgPool2d-176            [-1, 576, 1, 1]               0\n",
      "          Conv2d-177             [-1, 24, 1, 1]          13,848\n",
      "            SiLU-178             [-1, 24, 1, 1]               0\n",
      "          Conv2d-179            [-1, 576, 1, 1]          14,400\n",
      "         Sigmoid-180            [-1, 576, 1, 1]               0\n",
      "SqueezeExcitation-181          [-1, 576, 14, 14]               0\n",
      "          Conv2d-182           [-1, 96, 14, 14]          55,296\n",
      "     BatchNorm2d-183           [-1, 96, 14, 14]             192\n",
      " StochasticDepth-184           [-1, 96, 14, 14]               0\n",
      "          MBConv-185           [-1, 96, 14, 14]               0\n",
      "          Conv2d-186          [-1, 576, 14, 14]          55,296\n",
      "     BatchNorm2d-187          [-1, 576, 14, 14]           1,152\n",
      "            SiLU-188          [-1, 576, 14, 14]               0\n",
      "          Conv2d-189          [-1, 576, 14, 14]           5,184\n",
      "     BatchNorm2d-190          [-1, 576, 14, 14]           1,152\n",
      "            SiLU-191          [-1, 576, 14, 14]               0\n",
      "AdaptiveAvgPool2d-192            [-1, 576, 1, 1]               0\n",
      "          Conv2d-193             [-1, 24, 1, 1]          13,848\n",
      "            SiLU-194             [-1, 24, 1, 1]               0\n",
      "          Conv2d-195            [-1, 576, 1, 1]          14,400\n",
      "         Sigmoid-196            [-1, 576, 1, 1]               0\n",
      "SqueezeExcitation-197          [-1, 576, 14, 14]               0\n",
      "          Conv2d-198           [-1, 96, 14, 14]          55,296\n",
      "     BatchNorm2d-199           [-1, 96, 14, 14]             192\n",
      " StochasticDepth-200           [-1, 96, 14, 14]               0\n",
      "          MBConv-201           [-1, 96, 14, 14]               0\n",
      "          Conv2d-202          [-1, 576, 14, 14]          55,296\n",
      "     BatchNorm2d-203          [-1, 576, 14, 14]           1,152\n",
      "            SiLU-204          [-1, 576, 14, 14]               0\n",
      "          Conv2d-205          [-1, 576, 14, 14]          14,400\n",
      "     BatchNorm2d-206          [-1, 576, 14, 14]           1,152\n",
      "            SiLU-207          [-1, 576, 14, 14]               0\n",
      "AdaptiveAvgPool2d-208            [-1, 576, 1, 1]               0\n",
      "          Conv2d-209             [-1, 24, 1, 1]          13,848\n",
      "            SiLU-210             [-1, 24, 1, 1]               0\n",
      "          Conv2d-211            [-1, 576, 1, 1]          14,400\n",
      "         Sigmoid-212            [-1, 576, 1, 1]               0\n",
      "SqueezeExcitation-213          [-1, 576, 14, 14]               0\n",
      "          Conv2d-214          [-1, 136, 14, 14]          78,336\n",
      "     BatchNorm2d-215          [-1, 136, 14, 14]             272\n",
      "          MBConv-216          [-1, 136, 14, 14]               0\n",
      "          Conv2d-217          [-1, 816, 14, 14]         110,976\n",
      "     BatchNorm2d-218          [-1, 816, 14, 14]           1,632\n",
      "            SiLU-219          [-1, 816, 14, 14]               0\n",
      "          Conv2d-220          [-1, 816, 14, 14]          20,400\n",
      "     BatchNorm2d-221          [-1, 816, 14, 14]           1,632\n",
      "            SiLU-222          [-1, 816, 14, 14]               0\n",
      "AdaptiveAvgPool2d-223            [-1, 816, 1, 1]               0\n",
      "          Conv2d-224             [-1, 34, 1, 1]          27,778\n",
      "            SiLU-225             [-1, 34, 1, 1]               0\n",
      "          Conv2d-226            [-1, 816, 1, 1]          28,560\n",
      "         Sigmoid-227            [-1, 816, 1, 1]               0\n",
      "SqueezeExcitation-228          [-1, 816, 14, 14]               0\n",
      "          Conv2d-229          [-1, 136, 14, 14]         110,976\n",
      "     BatchNorm2d-230          [-1, 136, 14, 14]             272\n",
      " StochasticDepth-231          [-1, 136, 14, 14]               0\n",
      "          MBConv-232          [-1, 136, 14, 14]               0\n",
      "          Conv2d-233          [-1, 816, 14, 14]         110,976\n",
      "     BatchNorm2d-234          [-1, 816, 14, 14]           1,632\n",
      "            SiLU-235          [-1, 816, 14, 14]               0\n",
      "          Conv2d-236          [-1, 816, 14, 14]          20,400\n",
      "     BatchNorm2d-237          [-1, 816, 14, 14]           1,632\n",
      "            SiLU-238          [-1, 816, 14, 14]               0\n",
      "AdaptiveAvgPool2d-239            [-1, 816, 1, 1]               0\n",
      "          Conv2d-240             [-1, 34, 1, 1]          27,778\n",
      "            SiLU-241             [-1, 34, 1, 1]               0\n",
      "          Conv2d-242            [-1, 816, 1, 1]          28,560\n",
      "         Sigmoid-243            [-1, 816, 1, 1]               0\n",
      "SqueezeExcitation-244          [-1, 816, 14, 14]               0\n",
      "          Conv2d-245          [-1, 136, 14, 14]         110,976\n",
      "     BatchNorm2d-246          [-1, 136, 14, 14]             272\n",
      " StochasticDepth-247          [-1, 136, 14, 14]               0\n",
      "          MBConv-248          [-1, 136, 14, 14]               0\n",
      "          Conv2d-249          [-1, 816, 14, 14]         110,976\n",
      "     BatchNorm2d-250          [-1, 816, 14, 14]           1,632\n",
      "            SiLU-251          [-1, 816, 14, 14]               0\n",
      "          Conv2d-252          [-1, 816, 14, 14]          20,400\n",
      "     BatchNorm2d-253          [-1, 816, 14, 14]           1,632\n",
      "            SiLU-254          [-1, 816, 14, 14]               0\n",
      "AdaptiveAvgPool2d-255            [-1, 816, 1, 1]               0\n",
      "          Conv2d-256             [-1, 34, 1, 1]          27,778\n",
      "            SiLU-257             [-1, 34, 1, 1]               0\n",
      "          Conv2d-258            [-1, 816, 1, 1]          28,560\n",
      "         Sigmoid-259            [-1, 816, 1, 1]               0\n",
      "SqueezeExcitation-260          [-1, 816, 14, 14]               0\n",
      "          Conv2d-261          [-1, 136, 14, 14]         110,976\n",
      "     BatchNorm2d-262          [-1, 136, 14, 14]             272\n",
      " StochasticDepth-263          [-1, 136, 14, 14]               0\n",
      "          MBConv-264          [-1, 136, 14, 14]               0\n",
      "          Conv2d-265          [-1, 816, 14, 14]         110,976\n",
      "     BatchNorm2d-266          [-1, 816, 14, 14]           1,632\n",
      "            SiLU-267          [-1, 816, 14, 14]               0\n",
      "          Conv2d-268          [-1, 816, 14, 14]          20,400\n",
      "     BatchNorm2d-269          [-1, 816, 14, 14]           1,632\n",
      "            SiLU-270          [-1, 816, 14, 14]               0\n",
      "AdaptiveAvgPool2d-271            [-1, 816, 1, 1]               0\n",
      "          Conv2d-272             [-1, 34, 1, 1]          27,778\n",
      "            SiLU-273             [-1, 34, 1, 1]               0\n",
      "          Conv2d-274            [-1, 816, 1, 1]          28,560\n",
      "         Sigmoid-275            [-1, 816, 1, 1]               0\n",
      "SqueezeExcitation-276          [-1, 816, 14, 14]               0\n",
      "          Conv2d-277          [-1, 136, 14, 14]         110,976\n",
      "     BatchNorm2d-278          [-1, 136, 14, 14]             272\n",
      " StochasticDepth-279          [-1, 136, 14, 14]               0\n",
      "          MBConv-280          [-1, 136, 14, 14]               0\n",
      "          Conv2d-281          [-1, 816, 14, 14]         110,976\n",
      "     BatchNorm2d-282          [-1, 816, 14, 14]           1,632\n",
      "            SiLU-283          [-1, 816, 14, 14]               0\n",
      "          Conv2d-284            [-1, 816, 7, 7]          20,400\n",
      "     BatchNorm2d-285            [-1, 816, 7, 7]           1,632\n",
      "            SiLU-286            [-1, 816, 7, 7]               0\n",
      "AdaptiveAvgPool2d-287            [-1, 816, 1, 1]               0\n",
      "          Conv2d-288             [-1, 34, 1, 1]          27,778\n",
      "            SiLU-289             [-1, 34, 1, 1]               0\n",
      "          Conv2d-290            [-1, 816, 1, 1]          28,560\n",
      "         Sigmoid-291            [-1, 816, 1, 1]               0\n",
      "SqueezeExcitation-292            [-1, 816, 7, 7]               0\n",
      "          Conv2d-293            [-1, 232, 7, 7]         189,312\n",
      "     BatchNorm2d-294            [-1, 232, 7, 7]             464\n",
      "          MBConv-295            [-1, 232, 7, 7]               0\n",
      "          Conv2d-296           [-1, 1392, 7, 7]         322,944\n",
      "     BatchNorm2d-297           [-1, 1392, 7, 7]           2,784\n",
      "            SiLU-298           [-1, 1392, 7, 7]               0\n",
      "          Conv2d-299           [-1, 1392, 7, 7]          34,800\n",
      "     BatchNorm2d-300           [-1, 1392, 7, 7]           2,784\n",
      "            SiLU-301           [-1, 1392, 7, 7]               0\n",
      "AdaptiveAvgPool2d-302           [-1, 1392, 1, 1]               0\n",
      "          Conv2d-303             [-1, 58, 1, 1]          80,794\n",
      "            SiLU-304             [-1, 58, 1, 1]               0\n",
      "          Conv2d-305           [-1, 1392, 1, 1]          82,128\n",
      "         Sigmoid-306           [-1, 1392, 1, 1]               0\n",
      "SqueezeExcitation-307           [-1, 1392, 7, 7]               0\n",
      "          Conv2d-308            [-1, 232, 7, 7]         322,944\n",
      "     BatchNorm2d-309            [-1, 232, 7, 7]             464\n",
      " StochasticDepth-310            [-1, 232, 7, 7]               0\n",
      "          MBConv-311            [-1, 232, 7, 7]               0\n",
      "          Conv2d-312           [-1, 1392, 7, 7]         322,944\n",
      "     BatchNorm2d-313           [-1, 1392, 7, 7]           2,784\n",
      "            SiLU-314           [-1, 1392, 7, 7]               0\n",
      "          Conv2d-315           [-1, 1392, 7, 7]          34,800\n",
      "     BatchNorm2d-316           [-1, 1392, 7, 7]           2,784\n",
      "            SiLU-317           [-1, 1392, 7, 7]               0\n",
      "AdaptiveAvgPool2d-318           [-1, 1392, 1, 1]               0\n",
      "          Conv2d-319             [-1, 58, 1, 1]          80,794\n",
      "            SiLU-320             [-1, 58, 1, 1]               0\n",
      "          Conv2d-321           [-1, 1392, 1, 1]          82,128\n",
      "         Sigmoid-322           [-1, 1392, 1, 1]               0\n",
      "SqueezeExcitation-323           [-1, 1392, 7, 7]               0\n",
      "          Conv2d-324            [-1, 232, 7, 7]         322,944\n",
      "     BatchNorm2d-325            [-1, 232, 7, 7]             464\n",
      " StochasticDepth-326            [-1, 232, 7, 7]               0\n",
      "          MBConv-327            [-1, 232, 7, 7]               0\n",
      "          Conv2d-328           [-1, 1392, 7, 7]         322,944\n",
      "     BatchNorm2d-329           [-1, 1392, 7, 7]           2,784\n",
      "            SiLU-330           [-1, 1392, 7, 7]               0\n",
      "          Conv2d-331           [-1, 1392, 7, 7]          34,800\n",
      "     BatchNorm2d-332           [-1, 1392, 7, 7]           2,784\n",
      "            SiLU-333           [-1, 1392, 7, 7]               0\n",
      "AdaptiveAvgPool2d-334           [-1, 1392, 1, 1]               0\n",
      "          Conv2d-335             [-1, 58, 1, 1]          80,794\n",
      "            SiLU-336             [-1, 58, 1, 1]               0\n",
      "          Conv2d-337           [-1, 1392, 1, 1]          82,128\n",
      "         Sigmoid-338           [-1, 1392, 1, 1]               0\n",
      "SqueezeExcitation-339           [-1, 1392, 7, 7]               0\n",
      "          Conv2d-340            [-1, 232, 7, 7]         322,944\n",
      "     BatchNorm2d-341            [-1, 232, 7, 7]             464\n",
      " StochasticDepth-342            [-1, 232, 7, 7]               0\n",
      "          MBConv-343            [-1, 232, 7, 7]               0\n",
      "          Conv2d-344           [-1, 1392, 7, 7]         322,944\n",
      "     BatchNorm2d-345           [-1, 1392, 7, 7]           2,784\n",
      "            SiLU-346           [-1, 1392, 7, 7]               0\n",
      "          Conv2d-347           [-1, 1392, 7, 7]          34,800\n",
      "     BatchNorm2d-348           [-1, 1392, 7, 7]           2,784\n",
      "            SiLU-349           [-1, 1392, 7, 7]               0\n",
      "AdaptiveAvgPool2d-350           [-1, 1392, 1, 1]               0\n",
      "          Conv2d-351             [-1, 58, 1, 1]          80,794\n",
      "            SiLU-352             [-1, 58, 1, 1]               0\n",
      "          Conv2d-353           [-1, 1392, 1, 1]          82,128\n",
      "         Sigmoid-354           [-1, 1392, 1, 1]               0\n",
      "SqueezeExcitation-355           [-1, 1392, 7, 7]               0\n",
      "          Conv2d-356            [-1, 232, 7, 7]         322,944\n",
      "     BatchNorm2d-357            [-1, 232, 7, 7]             464\n",
      " StochasticDepth-358            [-1, 232, 7, 7]               0\n",
      "          MBConv-359            [-1, 232, 7, 7]               0\n",
      "          Conv2d-360           [-1, 1392, 7, 7]         322,944\n",
      "     BatchNorm2d-361           [-1, 1392, 7, 7]           2,784\n",
      "            SiLU-362           [-1, 1392, 7, 7]               0\n",
      "          Conv2d-363           [-1, 1392, 7, 7]          34,800\n",
      "     BatchNorm2d-364           [-1, 1392, 7, 7]           2,784\n",
      "            SiLU-365           [-1, 1392, 7, 7]               0\n",
      "AdaptiveAvgPool2d-366           [-1, 1392, 1, 1]               0\n",
      "          Conv2d-367             [-1, 58, 1, 1]          80,794\n",
      "            SiLU-368             [-1, 58, 1, 1]               0\n",
      "          Conv2d-369           [-1, 1392, 1, 1]          82,128\n",
      "         Sigmoid-370           [-1, 1392, 1, 1]               0\n",
      "SqueezeExcitation-371           [-1, 1392, 7, 7]               0\n",
      "          Conv2d-372            [-1, 232, 7, 7]         322,944\n",
      "     BatchNorm2d-373            [-1, 232, 7, 7]             464\n",
      " StochasticDepth-374            [-1, 232, 7, 7]               0\n",
      "          MBConv-375            [-1, 232, 7, 7]               0\n",
      "          Conv2d-376           [-1, 1392, 7, 7]         322,944\n",
      "     BatchNorm2d-377           [-1, 1392, 7, 7]           2,784\n",
      "            SiLU-378           [-1, 1392, 7, 7]               0\n",
      "          Conv2d-379           [-1, 1392, 7, 7]          12,528\n",
      "     BatchNorm2d-380           [-1, 1392, 7, 7]           2,784\n",
      "            SiLU-381           [-1, 1392, 7, 7]               0\n",
      "AdaptiveAvgPool2d-382           [-1, 1392, 1, 1]               0\n",
      "          Conv2d-383             [-1, 58, 1, 1]          80,794\n",
      "            SiLU-384             [-1, 58, 1, 1]               0\n",
      "          Conv2d-385           [-1, 1392, 1, 1]          82,128\n",
      "         Sigmoid-386           [-1, 1392, 1, 1]               0\n",
      "SqueezeExcitation-387           [-1, 1392, 7, 7]               0\n",
      "          Conv2d-388            [-1, 384, 7, 7]         534,528\n",
      "     BatchNorm2d-389            [-1, 384, 7, 7]             768\n",
      "          MBConv-390            [-1, 384, 7, 7]               0\n",
      "          Conv2d-391           [-1, 2304, 7, 7]         884,736\n",
      "     BatchNorm2d-392           [-1, 2304, 7, 7]           4,608\n",
      "            SiLU-393           [-1, 2304, 7, 7]               0\n",
      "          Conv2d-394           [-1, 2304, 7, 7]          20,736\n",
      "     BatchNorm2d-395           [-1, 2304, 7, 7]           4,608\n",
      "            SiLU-396           [-1, 2304, 7, 7]               0\n",
      "AdaptiveAvgPool2d-397           [-1, 2304, 1, 1]               0\n",
      "          Conv2d-398             [-1, 96, 1, 1]         221,280\n",
      "            SiLU-399             [-1, 96, 1, 1]               0\n",
      "          Conv2d-400           [-1, 2304, 1, 1]         223,488\n",
      "         Sigmoid-401           [-1, 2304, 1, 1]               0\n",
      "SqueezeExcitation-402           [-1, 2304, 7, 7]               0\n",
      "          Conv2d-403            [-1, 384, 7, 7]         884,736\n",
      "     BatchNorm2d-404            [-1, 384, 7, 7]             768\n",
      " StochasticDepth-405            [-1, 384, 7, 7]               0\n",
      "          MBConv-406            [-1, 384, 7, 7]               0\n",
      "          Conv2d-407           [-1, 1536, 7, 7]         589,824\n",
      "     BatchNorm2d-408           [-1, 1536, 7, 7]           3,072\n",
      "            SiLU-409           [-1, 1536, 7, 7]               0\n",
      "AdaptiveAvgPool2d-410           [-1, 1536, 1, 1]               0\n",
      "        Identity-411                 [-1, 1536]               0\n",
      "    EfficientNet-412                 [-1, 1536]               0\n",
      "     BatchNorm1d-413                 [-1, 1536]           3,072\n",
      "          Linear-414                  [-1, 256]         393,472\n",
      "            ReLU-415                  [-1, 256]               0\n",
      "         Dropout-416                  [-1, 256]               0\n",
      "          Linear-417                   [-1, 74]          19,018\n",
      "================================================================\n",
      "Total params: 11,111,794\n",
      "Trainable params: 415,562\n",
      "Non-trainable params: 10,696,232\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 341.36\n",
      "Params size (MB): 42.39\n",
      "Estimated Total Size (MB): 384.32\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "class EfficientNetB3Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EfficientNetB3Model, self).__init__()\n",
    "        \n",
    "        # Load EfficientNet-B3 base model (without pre-trained weights)\n",
    "        self.base_model = models.efficientnet_b3(weights=None)  # No pretrained weights at first\n",
    "        self.base_model.classifier = nn.Identity()  # Remove final classification layer\n",
    "        \n",
    "        # Load pre-trained weights\n",
    "        state_dict = torch.load(\"preTrained/efficientnet_b3_weights.pth\", weights_only=True)\n",
    "        # Remove classifier weights from state_dict to avoid mismatch\n",
    "        state_dict = {k: v for k, v in state_dict.items() if 'classifier' not in k}\n",
    "        \n",
    "        # Load filtered state_dict into the model\n",
    "        self.base_model.load_state_dict(state_dict)\n",
    "        \n",
    "        # Custom layers added after the base model\n",
    "        self.batch_norm = nn.BatchNorm1d(1536)  # Output size of EfficientNet-B3 before classifier is 1536\n",
    "        self.fc1 = nn.Linear(1536, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(256, 74)\n",
    "        \n",
    "        # Freeze base model layers if specified\n",
    "        for param in self.base_model.parameters():\n",
    "                param.requires_grad = False  # Freeze base model layers\n",
    "\n",
    "        # Enable mixed-precision training if needed (requires PyTorch AMP)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Automatic Mixed Precision (AMP) context manager for lower precision computations\n",
    "        with torch.amp.autocast(device_type='cuda'):  # Updated usage for AMP\n",
    "                x = self.base_model(x)  # Get features from EfficientNet-B3\n",
    "                x = self.batch_norm(x)   # Apply batch normalization\n",
    "                x = self.fc1(x)          # Apply fully connected layer 1\n",
    "                x = self.relu(x)         # ReLU activation\n",
    "                x = self.dropout(x)      # Apply dropout\n",
    "                x = self.fc2(x)          # Output layer\n",
    "        return x\n",
    "\n",
    "# Initialize the model with the option to use AMP (mixed precision)\n",
    "use_amp = True  # Set to True if you want to use mixed precision\n",
    "model = EfficientNetB3Model()\n",
    "\n",
    "# Assuming 'device' is the variable where the device is specified, e.g., 'cuda' or 'cpu'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the appropriate device (GPU if available, otherwise CPU)\n",
    "model = model.to(device)\n",
    "summary(model, input_size=(3, 224, 224))  # Input size (C, H, W) for EfficientNet-B3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb5ff936-a65b-4f08-842a-d519a1742487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "class DS_01(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.desc = \"resize[224, 224] -> transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\"\n",
    "        self.root_dir = root_dir\n",
    "        \n",
    "        # Define default transformations (resize, to tensor, and normalization)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),  # Resize to 224x224 pixels\n",
    "            transforms.ToTensor(),  # Convert image to tensor\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "        ])\n",
    "        \n",
    "        # Get all class names (subfolder names)\n",
    "        self.class_names = sorted(os.listdir(root_dir))\n",
    "        \n",
    "        # Create a mapping from class name to class index\n",
    "        self.class_to_idx = {class_name: idx for idx, class_name in enumerate(self.class_names)}\n",
    "        \n",
    "        # Collect all image paths and their corresponding class labels\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for class_name in self.class_names:\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            if os.path.isdir(class_dir):\n",
    "                for filename in os.listdir(class_dir):\n",
    "                    if filename.endswith(('.png', '.jpg', '.jpeg')):  # You can adjust extensions\n",
    "                        self.image_paths.append(os.path.join(class_dir, filename))\n",
    "                        self.labels.append(self.class_to_idx[class_name])\n",
    "    def collate_fn(batch):\n",
    "        inputs, labels = zip(*batch)  # Unzip the batch into inputs and labels\n",
    "        inputs = torch.stack(inputs, dim=0)  # Stack inputs to create a tensor\n",
    "        labels = torch.tensor(labels, dtype=torch.long)  # Convert labels to tensor of type long (integer)\n",
    "        def one_hot_encode(labels, num_classes=74):\n",
    "            # Convert integer labels to one-hot encoded labels\n",
    "            return torch.eye(num_classes)[labels]\n",
    "        # Apply one-hot encoding to the labels\n",
    "        labels = one_hot_encode(labels)\n",
    "        \n",
    "        return inputs, labels\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")  # Convert to RGB to ensure consistent color channels\n",
    "        \n",
    "        # Get the label for the image\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Apply transformations if provided\n",
    "        img = self.transform(img)\n",
    "        \n",
    "        return img, label\n",
    "\n",
    "\n",
    "# Example usage of the CustomImageDataset class\n",
    "\n",
    "# Initialize the dataset\n",
    "dataset = DS_01(root_dir='../DataSets/AirCrafts_2/Training/')\n",
    "\n",
    "# # Example: Accessing the first image and label\n",
    "# img, label = dataset[0]\n",
    "\n",
    "# print(f\"Image size: {img.size()}\")\n",
    "# print(f\"Label: {label}\")\n",
    "\n",
    "# Optional: Create a DataLoader to load the dataset in batches\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Example: Iterating over the dataloader\n",
    "for images, labels in dataloader:\n",
    "    print(images.shape)  # Image batch size: [32, 3, 224, 224]\n",
    "    print(labels.shape)  # Label batch size: [32]\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b47e29f-609d-4c89-b0b8-d5ec81d0dc5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'resize[224, 224] -> transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dabeca62-5582-483a-9cd5-ed126d33709d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration file is saved at internal/Test/test_c.json\n",
      "History will be saved at internal/Test/test_h.csv\n",
      "Weights will be saved at internal/Test/test_w.pth\n",
      "Data loaders are successfully created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|| 364/364 [00:39<00:00,  9.29it/s, accuracy=0.0179, loss\n",
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 4.4727, Train Accuracy: 0.02, Val Loss: 6.6808, Val Accuracy: 0.05\n",
      "Best Model Weights Updated: Epoch 1 - Val Loss: 6.680778130070194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|| 364/364 [00:16<00:00, 21.52it/s, accuracy=0.0604, loss\n",
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 4.0723, Train Accuracy: 0.06, Val Loss: 4.3245, Val Accuracy: 0.11\n",
      "Best Model Weights Updated: Epoch 2 - Val Loss: 4.324488139742023\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "P = test_mods(model=model,dataset=DS_01,\n",
    "              train_data_src=\"../DataSets/AirCrafts-Copy_trn/\",\n",
    "              valid_data_src=\"../DataSets/AirCrafts-Copy_vld/\",\n",
    "              train_batch_size=2,\n",
    "              valid_batch_size=2,\n",
    "              prepare=True)\n",
    "P.train(num_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcd0413-635c-42fd-b193-44240545fd99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a6e624-cc80-4a69-9469-afe49970b05a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbaf5ad-285d-4439-b29c-3c6bccb66a2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b262a08-2c28-48a6-b74c-b1bae467a0dd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from PyTorchLabFlow import set_default_config\n",
    "set_default_config({\n",
    "    \"accuracy_loc\": \"Libs.accuracies.MultiAcc\",\n",
    "    \"loss_loc\": \"Libs.losses.CrossEn\",\n",
    "    \"optimizer_loc\": \"Libs.optimizers.OptAdamax\",\n",
    "    \"train_data_src\": \"../DataSets/AirCrafts_2/Training/\",\n",
    "    \"valid_data_src\": \"../DataSets/AirCrafts_2/Test/\",\n",
    "    \"train_batch_size\": 32,\n",
    "    \"valid_batch_size\": 32\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ff68efe-76c5-4724-b300-007a53af8873",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to C:\\Users\\LENOVO/.cache\\torch\\hub\\checkpoints\\vgg19-dcbb9e9d.pth\n",
      " 21%|                         | 118M/548M [16:28<1:00:14, 125kB/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid hash value (expected \"dcbb9e9d\", got \"25e70540fc61802670dcde10255abaf4b7a02b3802e995f61d649e31f9f6db63\")",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load the pre-trained VGG19 model\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m vgg19 \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mvgg19(weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVGG19_Weights.DEFAULT\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Step 2: Save the model weights (state_dict) to a file\u001b[39;00m\n\u001b[0;32m      8\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(vgg19\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreTrained/vgg19.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\AnnA\\envs\\diat\\Lib\\site-packages\\torchvision\\models\\_utils.py:142\u001b[0m, in \u001b[0;36mkwonly_to_pos_or_kw.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    135\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    136\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msequence_to_str(\u001b[38;5;28mtuple\u001b[39m(keyword_only_kwargs\u001b[38;5;241m.\u001b[39mkeys()),\u001b[38;5;250m \u001b[39mseparate_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand \u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as positional \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    137\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    138\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    139\u001b[0m     )\n\u001b[0;32m    140\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate(keyword_only_kwargs)\n\u001b[1;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\AnnA\\envs\\diat\\Lib\\site-packages\\torchvision\\models\\_utils.py:228\u001b[0m, in \u001b[0;36mhandle_legacy_interface.<locals>.outer_wrapper.<locals>.inner_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[pretrained_param]\n\u001b[0;32m    226\u001b[0m     kwargs[weights_param] \u001b[38;5;241m=\u001b[39m default_weights_arg\n\u001b[1;32m--> 228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m builder(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\AnnA\\envs\\diat\\Lib\\site-packages\\torchvision\\models\\vgg.py:485\u001b[0m, in \u001b[0;36mvgg19\u001b[1;34m(weights, progress, **kwargs)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"VGG-19 from `Very Deep Convolutional Networks for Large-Scale Image Recognition <https://arxiv.org/abs/1409.1556>`__.\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \n\u001b[0;32m    467\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;124;03m    :members:\u001b[39;00m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    483\u001b[0m weights \u001b[38;5;241m=\u001b[39m VGG19_Weights\u001b[38;5;241m.\u001b[39mverify(weights)\n\u001b[1;32m--> 485\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _vgg(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m, weights, progress, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\AnnA\\envs\\diat\\Lib\\site-packages\\torchvision\\models\\vgg.py:105\u001b[0m, in \u001b[0;36m_vgg\u001b[1;34m(cfg, batch_norm, weights, progress, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m model \u001b[38;5;241m=\u001b[39m VGG(make_layers(cfgs[cfg], batch_norm\u001b[38;5;241m=\u001b[39mbatch_norm), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 105\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(weights\u001b[38;5;241m.\u001b[39mget_state_dict(progress\u001b[38;5;241m=\u001b[39mprogress, check_hash\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mD:\\AnnA\\envs\\diat\\Lib\\site-packages\\torchvision\\models\\_api.py:90\u001b[0m, in \u001b[0;36mWeightsEnum.get_state_dict\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state_dict\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Mapping[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m---> 90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m load_state_dict_from_url(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\AnnA\\envs\\diat\\Lib\\site-packages\\torch\\hub.py:867\u001b[0m, in \u001b[0;36mload_state_dict_from_url\u001b[1;34m(url, model_dir, map_location, progress, check_hash, file_name, weights_only)\u001b[0m\n\u001b[0;32m    865\u001b[0m         r \u001b[38;5;241m=\u001b[39m HASH_REGEX\u001b[38;5;241m.\u001b[39msearch(filename)  \u001b[38;5;66;03m# r is Optional[Match[str]]\u001b[39;00m\n\u001b[0;32m    866\u001b[0m         hash_prefix \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 867\u001b[0m     download_url_to_file(url, cached_file, hash_prefix, progress\u001b[38;5;241m=\u001b[39mprogress)\n\u001b[0;32m    869\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_legacy_zip_format(cached_file):\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_zip_load(cached_file, model_dir, map_location, weights_only)\n",
      "File \u001b[1;32mD:\\AnnA\\envs\\diat\\Lib\\site-packages\\torch\\hub.py:756\u001b[0m, in \u001b[0;36mdownload_url_to_file\u001b[1;34m(url, dst, hash_prefix, progress)\u001b[0m\n\u001b[0;32m    754\u001b[0m         digest \u001b[38;5;241m=\u001b[39m sha256\u001b[38;5;241m.\u001b[39mhexdigest()  \u001b[38;5;66;03m# type: ignore[possibly-undefined]\u001b[39;00m\n\u001b[0;32m    755\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m digest[: \u001b[38;5;28mlen\u001b[39m(hash_prefix)] \u001b[38;5;241m!=\u001b[39m hash_prefix:\n\u001b[1;32m--> 756\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    757\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minvalid hash value (expected \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhash_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdigest\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    758\u001b[0m             )\n\u001b[0;32m    759\u001b[0m     shutil\u001b[38;5;241m.\u001b[39mmove(f\u001b[38;5;241m.\u001b[39mname, dst)\n\u001b[0;32m    760\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: invalid hash value (expected \"dcbb9e9d\", got \"25e70540fc61802670dcde10255abaf4b7a02b3802e995f61d649e31f9f6db63\")"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "# Load the pre-trained VGG19 model\n",
    "vgg19 = models.vgg19(weights='VGG19_Weights.DEFAULT')\n",
    "\n",
    "# Step 2: Save the model weights (state_dict) to a file\n",
    "torch.save(vgg19.state_dict(), \"preTrained/vgg19.pth\")\n",
    "\n",
    "print(\"vgg19 weights saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a19e27d-d4bc-4b35-b528-03aff61726ee",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b3_rwightman-b3899882.pth\" to C:\\Users\\LENOVO/.cache\\torch\\hub\\checkpoints\\efficientnet_b3_rwightman-b3899882.pth\n",
      "100%|| 47.2M/47.2M [02:10<00:00, 380kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EfficientNet-B3 weights saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "# Step 1: Load the pre-trained EfficientNet-B3 model\n",
    "efficientnet_b3 = models.efficientnet_b3(pretrained=True)\n",
    "\n",
    "# Step 2: Save the model weights (state_dict) to a file\n",
    "torch.save(efficientnet_b3.state_dict(), \"efficientnet_b3_weights.pth\")\n",
    "\n",
    "print(\"EfficientNet-B3 weights saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
